[TOC]

# 概率图模型

## 隐马尔可夫模型

​		机器学习最重要的任务，是根据一些已观察到的证据(例如训练样本)来对感兴趣的未知变量(例如类别标记)进行估计和推测。概率模型 (probabilistic model) 提供了一种描述框架，将学习任务归结于计算变量的概率分布。在概率模型中，利用已知变量推测未知变量的分布称为“ 推断” (inference)，其核心是如何基于可观测变量推测出未知变量的条件分布。具体来说，假定所关心的变量集合为 $Y$，可观测变量集合为 $O$，其他变量的集合为 $R$，“生成式” (generative) 模型考虑联合分布 $P(Y,R,O)$，“判别式” (discriminative) 模型考虑条件分布 $P(Y,R\mid O)$。给定一组观测变量值，推断就是要由 $P(Y,R,O)$ 或 $P(Y,R\mid O)$ 得到条件概率分布 $P(Y\mid O)$。

​		直接利用概率求和规则消去变量 $R$ 显然不可行，因为即便每个变量仅有两种取值的简单问题，其复杂度已至少是 $O(2^{|Y|+|R|})$。另一方面，属性变量之间往往存在复杂的联系，因此概率模型的学习，即基于训练样本来估计变量分布的参数往往相当困难。为了便于研究高效的推断和学习算法，需有一套能简洁紧凑地表达变量间关系的工具。

​		概率图模型 (probabilistic graphical model) 是一类用图来表达变量相关关系的概率模型。它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，结点之间的边表示变量间的概率相关关系，即“变量关系图”。根据边的性质不同，概率图模型可大致分为两类：

- 第一类是使用有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网(Bayesian network)；
- 第二类是使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网(Markov network)。

>若变量间存在显式的因果关系，则常使用贝叶斯网；
>
>若变量间存在相关性，但难以获得显式的因果关系，则常使用马尔可夫网。

​		隐马尔可夫模型 (Hidden Markov Model，简称HMM) 是结构最简单的动态贝叶斯网(dynamic Bayesian network)，这是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用。

​		如下图所示，隐马尔可夫模型中的变量可分为两组。

![隐马尔可夫模型的图结构.png](https://github.com/Giyn/QG2020SummerTraining/blob/master/Pictures/HMM/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%BE%E7%BB%93%E6%9E%84.png?raw=true)

​		第一组是状态变量 $\{y_1,y_2,...,y_n\}$，其中 $y_i∈\gamma$ 表示第 $i$ 时刻的系统状态。通常假定状态变量是隐藏的、不可被观测的，因此状态变量亦称隐变量 (hidden variable)。第二组是观测变量 $\{x_1,x_2,...,x_n\}$，其中 $x_i∈\chi$ 表示第 $i$ 时刻的观测值。在隐马尔可夫模型中，系统通常在多个状态 $\{s_1,s_2,...,s_N\}$ 之间转换，因此状态变量 $y_i$ 的取值范围 $\gamma$ (称为状态空间) 通常是有 $N$ 个可能取值的离散空间。观测变量 $x_i$ 可以是离散型也可以是连续型，为便于讨论，我们仅考虑离散型观测变量，并假定其取值范围 $\chi$ 为 $\{o_1,o_2,...,o_M\}$。

​		上图中的箭头表示了变量间的依赖关系。在任一时刻，观测变量的取值仅依赖于状态变量，即 $x_t$ 由 $y_t$ 确定，与其他状态变量及观测变量的取值无关。同时，$t$ 时刻的状态 $y_t$ 仅依赖于 $t-1$ 时刻的状态 $y_{t-1}$，与其余 $n-2$ 个状态无关。这就是所谓的“马尔可夫链” (Markov chain)，即：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态。基于这种依赖关系，所有变量的联合概率分布为
$$
P(x_1,y_1,...,x_n,y_n)=P(y_1)P(x_1\mid y_1)\prod_{i=2}^nP(y_i\mid y_{i-1})P(x_i\mid y_i)\,\,.\tag{1}
$$
​		除了结构信息，欲确定一个隐马尔可夫模型还需以下三组参数：

- 状态转移概率：模型在各个状态间转换的概率，通常记为矩阵 $A=[a_{ij}]_{N×N}$，其中
  $$
  a_{ij}=P(y_{t+1}=s_j\mid y_t=s_i),\quad{}1≤i,j≤N,
  $$
  表示在任意时刻 $t$，若状态为 $s_i$，则在下一时刻状态为 $s_j$ 的概率。

- 输出观测概率：模型根据当前状态获得各个观测值的概率，通常记为矩阵 $B=[b_{ij}]_{N×M}$，其中
  $$
  b_{ij}= P(xt=oj|t=si)，
  1≤i≤N,1≤j≤M
  $$

  表示在任意时刻 $t$，若状态为 $s_i$，则观测值 $o_j$ 被获取的概率。

- 初始状态概率：模型在初始时刻各状态出现的概率，通常记为 $π=(\pi_1,\pi_2,...,\pi_N)$，其中

$$
\pi_i=P(y_1=s_i)\,\,,\quad\quad1\leq i\leq N
$$

​		表示模型的初始状态为 $s_i$ 的概率。

​		通过指定状态空间 $\gamma$、观测空间 $\chi$ 和上述三组参数，就能确定一个隐马尔可夫模型，通常用其参数 $λ= [A,B, π]$ 来指代。给定隐马尔可夫模型 $\lambda$，它按如下过程产生观测序列 $\{x_1,x_2,...,x_n\}$：

(1) 设置 $t=1$，并根据初始状态概率 $π$ 选择初始状态 $y_1$； 

(2) 根据状态 $y_t$ 和输出观测概率 $B$ 选择观测变量取值 $x_t$;

(3) 根据状态 $y_t$ 和状态转移矩阵 $A$ 转移模型状态，即确定 $y_{t+1}$; .

(4) 若 $t<n$，设置 $t=t+1$，并转到第 (2) 步，否则停止。

其中 $y_t∈\{s_1,s_2,...,s_N\}$ 和 $x_t∈\{o_1,o_2,...,o_M\}$ 分别为第t时刻的状态和观测值。

​		在实际应用中，人们常关注隐马尔可夫模型的三个基本问题：

- 给定模型 $λ=[A,B,π]$， 如何有效计算其产生观测序列 $x∈\{x_1,x_2,...,x_n\}$ 的概率 $P(x\mid\lambda)$？换言之，如何评估模型与观测序列之间的匹配程度？
- 给定模型 $λ=[A,B,π]$ 和观测序列 $x∈\{x_1,x_2,...,x_n\}$ ，如何找到与此观测序列最匹配的状态序列 $y=\{y_1,y_2,...,y_n\}$？换言之，如何根据观测序列推断出隐藏的模型状态?
- 给定观测序列 $x∈\{x_1,x_2,...,x_n\}$ ，如何调整模型参数 $λ=[A,B,π]$ 使得该序列出现的概率 $P(x\mid\lambda)$ 最大？换言之，如何训练模型使其能最好地描述观测数据？

​		上述问题在现实应用中非常重要。例如许多任务需根据以往的观测序列 $\{x_1,x_2,...,x_{n-1}\}$ 来推测当前时刻最有可能的观测值 $x_n$，这显然可转化为求取概率 $P(x\mid\lambda)$，即上述第一个问题；在语音识别等任务中，观测值为语音信号，隐藏状态为文字，目标就是根据观测信号来推断最有可能的状态序列 (即对应的文字)，即上述第二个问题；在大多数现实应用中，人工指定模型参数已变得越来越不可行，如何根据训练样本学得最优的模型参数，恰是上述第三个问题。值得庆幸的是，基于式 (1) 的条件独立性，隐马尔可夫模型的这三个问题均能被高效求解。



### $HMM$ 优缺点：

#### 优点：

- $HMM$是结构最简单的动态贝叶斯网。
- 解决了标注问题。

#### 缺点：

- 做了齐次马尔科夫假设及观测股利性假设，可能出现标记偏置。
- $HMM$ 只依赖于每一个状态和它对应的观察对象，但序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。
- 目标函数和预测目标函数不匹配。$HMM$ 学到的是状态和观察序列的联合分布 $P(Y,X)$，而预测问题中，我们需要的是条件概率 $P(Y\mid X)$。



## 马尔可夫随机场

​		马尔可夫随机场 (Markov Random Field，简称 $MRF$) 是典型的马尔可夫网，这是一种著名的无向图模型。图中每个结点表示一个或一组变量，结点之间的边表示两个变量之间的依赖关系。马尔可夫随机场有一组势函数 (potentialfunctions)，亦称“因子” (factor)，这是定义在变量子集上的非负实函数，主要用于定义概率分布函数。

​		下图显示出一个简单的马尔可夫随机场。对于图中结点的一个子集，若其中任意两结点间都有边连接，则称该结点子集为一个“团” (clique)。若在一个团中加入另外任何一个结点都不再形成团，则称该团为“极大团” (maximalclique)；换言之，极大团就是不能被其他团所包含的团。

例如，在下图中，$\{x_1,x_2\}$，$\{x_1,x_3\}$，$\{x_2,x_4\}$，$\{x_2,x_5\}$，$\{x_2,x_6\}$，$\{x_3,x_5\}$，$\{x_5,x_6\}$ 和 $\{x_2,x_5,x_6\}$ 都是团，并且除了 $\{x_2,x_5\}$，$\{x_2,x_6\}$ 和 $\{x_5,x_6\}$ 之外都是极大团；但是，因为 $x_2$ 和 $x_3$ 之间缺乏连接，$\{x_1,x_2,x_3\}$ 并不构成团。显然，每个结点至少出现在一个极大团中。

![一个简单的马尔可夫随机场.png](https://github.com/Giyn/QG2020SummerTraining/blob/master/Pictures/HMM/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%9A%8F%E6%9C%BA%E5%9C%BA.png?raw=true)

​		在马尔可夫随机场中，多个变量之间的联合概率分布能基于团分解为多个因子的乘积，每个因子仅与一个团相关。具体来说，对于 $n$ 个变量 $x=\{x_1,x_2,...,x_n\}$，所有团构成的集合为 $C$，与团 $Q∈C$ 对应的变量集合记为 $x_Q$，则联合概率 $P(x)$ 定义为
$$
P(x)=\frac{1}{Z}\prod_{Q∈C}\psi_Q(x_Q)\,\,,\tag{2}
$$
其中 $ψ_Q$ 为与团 $Q$ 对应的势函数，用于对团 $Q$ 中的变量关系进行建模，$Z=\sum_x\prod_{Q∈C}\psi_Q(x_Q)$ 为规范化因子，以确保 $P(x)$ 是被正确定义的概率。在实际应用中，精确计算 $Z$ 通常很困难，但许多任务往往并不需获得 $Z$ 的精确值。

​		显然，若变量个数较多，则团的数目将会很多 (例如，所有相互连接的两个变量都会构成团)，这就意味着式 (2) 会有很多乘积项，显然会给计算带来负担。注意到若团 $Q$ 不是极大团，则它必被一个极大团 $Q^*$ 所包含，即 $x_Q\subseteq x_{Q^*}$；这意味着变量 $x_Q$ 之间的关系不仅体现在势函数 $ψ_Q$ 中，还体现在 $ψ_{Q^*}$ 中。于是，联合概率 $P(x)$ 可基于极大团来定义。假定所有极大团构成的集合为 $C^*$，则有
$$
P(x)=\frac{1}{Z^*}\prod_{Q∈C^*}\psi_Q(x_Q)\,\,,\tag{3}
$$
其中 $Z^*=\sum_x\prod_{Q∈C}\psi_Q(x_Q)$ 为规范化因子。例如上图中 $x=\{x_1,x_2,...,x_6\}$，联合概率分布 $P(x)$ 定义为
$$
P(x)=\frac{1}{Z}\psi_{12}(x_1,x_2)\psi_{13}(x_1,x_3)\psi_{24}(x_2,x_4)\psi_{35}(x_3,x_5)\psi_{256}(x_2,x_5,x_6)\,\,,
$$
其中，势函数 $\psi_{256}(x_2,x_5,x_6)$ 定义在极大团 $\{x_2,x_5,x_6\}$ 上，由于它的存在，使我们不再需为团 $\{x_2,x_5\}$，$\{x_2,x_6\}$ 和 $\{x_5,x_6\}$ 构建势函数。

​		在马尔可夫随机场中如何得到“条件独立性”呢？同样借助“分离”的概念，如下图所示，若从结点集 $A$ 中的结点到 $B$ 中的结点都必须经过结点集 $C$ 中的结点，则称结点集 $A$ 和 $B$ 被结点集 $C$ 分离，$C$ 称为“分离集”(separatingset)。对马尔可夫随机场，有

- "全局马尔可夫性” (global Markov property)：给定两个变量子集的分离集，则这两个变量子集条件独立。

也就是说，下图中若令 $A, B$ 和 $C$ 对应的变量集分别为 $x_A, x_B$ 和 $x_C$，则 $x_A$ 和 $x_B$ 在给定 $x_C$ 的条件下独立，记为 $x_A⊥x_B\mid xc$。

![结点集A和B被结点集C分离.png](https://github.com/Giyn/QG2020SummerTraining/blob/master/Pictures/HMM/%E7%BB%93%E7%82%B9%E9%9B%86A%E5%92%8CB%E8%A2%AB%E7%BB%93%E7%82%B9%E9%9B%86C%E5%88%86%E7%A6%BB.png?raw=true)

​		下面我们做一个简单的验证。为便于讨论，我们令上图中的 $A, B$ 和 $C$ 分别对应单变量 $x_A, x_B$ 和 $x_C$，于是上图简化为下图。

![结点A和B被结点集C分离图的简化版.png](https://github.com/Giyn/QG2020SummerTraining/blob/master/Pictures/HMM/%E7%BB%93%E7%82%B9A%E5%92%8CB%E8%A2%AB%E7%BB%93%E7%82%B9%E9%9B%86C%E5%88%86%E7%A6%BB%E5%9B%BE%E7%9A%84%E7%AE%80%E5%8C%96%E7%89%88.png?raw=true)

​		对于上图，由式 (2) 可得联合概率
$$
P(x_A,x_B,x_C)=\frac{1}{Z}\psi_{AC}(x_A,x_C)\psi_{BC}(x_B,x_C)\,\,.\tag{4}
$$
基于条件概率的定义可得
$$
\begin{align}
P(x_A,x_B\mid x_C)&=\frac{P(x_A,x_B,x_C)}{P(x_C)}=\frac{P(x_A,x_B,x_C)}{\sum_{x_A'}\sum_{x_B'}P(x'_A,x'_B,x_C)}\\
&=\frac{\frac{1}{Z}\psi_{AC}(x_A,x_C)\psi_{BC}(x_B,x_C)}{\sum_{x'_A}\sum_{x_B'}\frac{1}{Z}\psi_{AC}(x'_A,x_C)\psi_{BC}(x'_B,x_C)}\\
&=\frac{\psi_{AC}(x_A,x_C)}{\sum_{x'_A}\psi_{AC}(x'_A,x_C)}·\frac{\psi_{BC}(x_B,x_C)}{\sum_{x'_B}\psi_{BC}(x'_B,x_C)}\,\,.\tag{5}
\end{align}
$$

$$
\begin{align}
P(x_A\mid x_C)&=\frac{P(x_A,x_C)}{P(x_C)}=\frac{\sum_{x'_B}P(x_A,x_B',x_C)}{\sum_{x_A'}\sum_{x_B'}P(x'_A,x'_B,x_C)}\\
&=\frac{\sum_{x'_B}\frac{1}{Z}\psi_{AC}(x_A,x_C)\psi_{BC}(x_B',x_C)}{\sum_{x'_A}\sum_{x_B'}\frac{1}{Z}\psi_{AC}(x'_A,x_C)\psi_{BC}(x'_B,x_C)}\\
&=\frac{\psi_{AC}(x_A,x_C)}{\sum_{x'_A}\psi_{AC}(x'_A,x_C)}\,\,.\tag{6}
\end{align}
$$

由式 (5) 和 (6) 可知
$$
P(x_A,x_B\mid x_C)=P(x_A\mid x_C)P(x_B\mid x_C)\,\,,\tag{7}
$$
即 $x_A$ 和 $x_B$ 在给定 $x_C$ 时条件独立。

​		由全局马尔可夫性可得到两个很有用的推论：

- 局部马尔可夫性 (local Markov property)：给定某变量的邻接变量，则该变量条件独立于其他变量。形式化地说，令 $V$ 为图的结点集，$n(v)$ 为结点 $v$ 在图上的邻接结点，$n^*(v)= n(v)\bigcup\, \{v\}$，有 $x_v⊥x_{V\backslash n^*(v)}\mid x_n(v)$。

- 成对马尔可夫性 (pairwise Markov property)：给定所有其他变量，两个非邻接变量条件独立。形式化地说，令图的结点集和边集分别为 $V$ 和 $E$，对图中的两个结点 $u$ 和 $v$，若 $\langle u,v\rangle\notin E$，则 $x_u⊥x_v\mid x_{V\backslash \langle u,v \rangle}$。

>某变量的所有邻接变量组成的集合称为该变量的“马尔可夫毯" (Markov blanket)。

​		现在我们来考察马尔可夫随机场中的势函数。显然，势函数 $ψ_Q(x_Q)$ 的作用是定量刻画变集 $x_Q$ 中变量之间的相关关系，它应该是非负函数，且在所偏好的变量取值上有较大函数值。例如，假定上图中的变量均为二值变量，若势函数为
$$
\psi_{AC}(x_A,x_C)=
\begin{cases}
1.5,\quad if\,\,x_A=x_C;\\
0.1,\quad otherwise\,\,,
\end{cases}
$$

$$
\psi_{BC}(x_B,x_C)=
\begin{cases}
0.2,\quad if\,\,x_B=x_C;\\
1.3,\quad otherwise\,\,,
\end{cases}
$$

则说明该模型偏好变量 $x_A$ 与 $x_C$ 拥有相同的取值，$x_B$ 与 $x_C$ 拥有不同的取值；换言之，在该模型中 $x_A$ 与 $x_C$ 正相关，$x_B$ 与 $x_C$ 负相关。结合式 (2) 易知，令 $x_A$ 与 $x_C$ 相同且 $x_B$ 与 $x_C$ 不同的变量值指派将取得较高的联合概率。

​		为了满足非负性，指数函数常被用于定义势函数，即
$$
\psi_Q(x_Q)=e^{-H_Q(x_Q)}\,\,.\tag{8}
$$
$H_Q(x_Q)$ 是一个定义在变量 $x_Q$ 上的实值函数，常见形式为
$$
H_Q(x_Q)=\sum_{u,v∈Q,u\neq v}\alpha_{uv}x_ux_v+\sum_{v∈Q}\beta_vx_v\,\,,\tag{9}
$$
其中 $\alpha_{uv}$ 和 $\beta_v$ 是参数。上式中的第二项仅考虑单结点，第一项则考虑每一对结点的关系。



## 条件随机场

​		条件随机场 (Conditional Random Field，简称 $CRF$)是一种判别式无向图模型。生成式模型是直接对联合分布进行建模，而判别式模型则是对条件分布进行建模。前面介绍的隐马尔可夫模型和马尔可夫随机场都是生成式模型，而条件随机场则是判别式模型。

>条件随机场可看作给定观测值的马尔可夫随机场，也可看作对率回归的扩展。

​		条件随机场试图对多个变量在给定观测值后的条件概率进行建模。具体来说，若令 $x=\{x_1,x_2,...,x_n\}$ 为观测序列，$y=\{y_1,y_2,...,y_n\}$ 为与之相应的标记序列，则条件随机场的目标是构建条件概率模型 $P(y\mid x)$。需注意的是，标记变量 $y$ 可以是结构型变量，即其分量之间具有某种相关性。例如在自然语言处理的词性标注任务中，观测数据为语句(即单词序列)，标记为相应的词性序列，具有线性序列结构，如下图 (a) 所示；在语法分析任务中，输出标记则是语法树，具有树形结构，如下图 (b) 所示。

![自然语言处理中的词性标注和语法分析任务.png](https://github.com/Giyn/QG2020SummerTraining/blob/master/Pictures/HMM/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E5%92%8C%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1.png?raw=true)

​		令 $G=\langle V,E\rangle$ 表示结点与标记变量 $y$ 中元素一一对应的无向图，$y_v$ 表示与结点 $v$ 对应的标记变量，$n(v)$ 表示结点 $v$ 的邻接结点，若图 $G$ 的每个变量 $y_v$ 都满足马尔可夫性，即
$$
P(y_v\mid x,y_{\,V\backslash\{v\}})=P(y_v\mid x,y_{n(v)})\,\,,\tag{10}
$$
则 $(y,x)$ 构成一个条件随机场。

​		理论上来说，图 $G$ 可具有任意结构，只要能表示标记变量之间的条件独立性关系即可。但在现实应用中，尤其是对标记序列建模时，最常用的仍是下图所示的链式结构，即“链式条件随机场” (chain-structured $CRF$)。下面主要讨论这种条件随机场。

![链式条件随机场的图结构.png](https://github.com/Giyn/QG2020SummerTraining/blob/master/Pictures/HMM/%E9%93%BE%E5%BC%8F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%9A%84%E5%9B%BE%E7%BB%93%E6%9E%84.png?raw=true)

​		与马尔可夫随机场定义联合概率的方式类似，条件随机场使用势函数和图结构上的团来定义条件概率 $P(y\mid x)$。给定观测序列 $x$，上图所示的链式条件随机场主要包含两种关于标记变量的团，即单个标记变量 $\{y_i\}$ 以及相邻的标记变量 $\{y_{i-1},y_i\}$。选择合适的势函数，即可得到形如式 (2) 的条件概率定义。在条件随机场中，通过选用指数势函数并引入特征函数 (feature function)，条件概率被定义为
$$
P(y\mid x)=\frac{1}{Z}exp\left(\sum_j\sum_{i=1}^{n-1}\lambda_jt_j(y_{i+1},y_i,x,i)+\sum_k\sum_{i=1}^{n}\mu_ks_k(y_i,x,i)\right)\,\,,\tag{11}
$$
其中 $t_j(y_{i+1},y_i,x,i)$ 是定义在观测序列的两个相邻标记位置上的转移特征函数 (transition feature function)，用于刻画相邻标记变量之间的相关关系以及观测序列对它们的影响，$s_k(y_i,x,i)$ 是定义在观测序列的标记位置 $i$ 上的状态特征函数 (status feature function)，用于刻画观测序列对标记变量的影响，$λ_j$ 和 $μ_k$ 为参数，$Z$ 为规范化因子，用于确保式 (11) 是正确定义的概率。
		显然，要使用条件随机场，还需定义合适的特征函数。特征函数通常是实值函数，以刻画数据的一些很可能成立或期望成立的经验特性。以上上图 (a) 的词性标注任务为例，若采用转移特征函数
$$
t_j(y_{i+1},y_i,x,i)=
\begin{cases}
1,\quad{}if\,\,y_{i+1}=[P],\,\,y_i=[V]\,\,and\,\,x_i="knock";\\
0,\quad{}otherwise,
\end{cases}
$$
则表示第 $i$ 个观测值 $x_i$ 为单词“knock"时，相应的标记 $y_i$ 和 $y_{i+1}$ 很可能分别为 $[V]$ 和 $[P]$。若采用状态特征函数
$$
s_k(y_i,x,i)=
\begin{cases}
1,\quad{}if\,\,y_i=[V]\,\,and\,\,x_i="knock";\\
0,\quad{}otherwise,
\end{cases}
$$
则表示观测值 $x_i$ 为单词“knock"时，它所对应的标记很可能为 $[V]$。

​		对比式 (11) 和 (2) 可看出，条件随机场和马尔可夫随机场均使用团上的势函数定义概率，两者在形式上没有显著区别；但条件随机场处理的是条件概率，而马尔可夫随机场处理的是联合概率。

---

Reference：《机器学习》