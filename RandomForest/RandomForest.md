## 使用随机森林算法处理Kaggle上的声呐数据集

在数据挖掘算法中有一种算法叫bagging，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合，而且是随机采样。

> 随机采样就是从训练集里采集固定个数的样本，但是每采集一个样本后，都将样本放回。

由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些。

随机森林是Bagging算法的进化版，它的思想仍然是bagging，但是进行了改进。

---

### RF的主要优点有：

- 训练可以高度并行化。
- 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
- 在训练后，可以给出各个特征对于输出的重要性。
- 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
- 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
- 对部分特征缺失不敏感。

---

### RF的主要缺点有：

- 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
- 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

---

> 随机森林需要调整的参数有：
>
> - 决策树的个数
> - 特征属性的个数
> - 递归次数（决策树的深度）

---

### 该项目分为如下步骤：

1. ##### 下载好数据

   [点击这里下载](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/)

2. #####  读取数据集csv文件

3. ##### 将特征转换为float类型

4. ##### 将数据集分成n份，用于交叉验证

5. ##### 随机采样

6. ##### 在指定特征个数下选取最优特征

7. ##### 构造决策树

8. ##### 创建随机森林

9. ##### 输入测试集并进行测试，输出预测结果

---

#### 训练部分：

​	取dataset中的m个feature来构造决策树，首先遍历m个feature中的每一个feature，再遍历每一行，通过spilt_loss函数（计算分割代价）来选取最优的特征及特征值，根据是否大于这个特征值进行分类（分成left，right两类），循环执行上述步骤，直至不可分或者达到递归限值（用来防止过拟合），最后得到一个决策树tree。

#### 测试部分：

​	对测试集的每一行进行判断，决策树tree是一个多层字典，每一层为一个二分类，将每一行按照决策树tree中的分类索引index一步一步向里层探索，直至不出现字典时探索结束，得到的值即为预测值。
