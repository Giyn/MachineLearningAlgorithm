[TOC]

# 集成学习

## 个体与集成

​	集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(multi-classifer system)、基于委员会的学习(committee-based learning)等。



​	下图显示出集成学习的一般结构：先产生一组“个体学习器”(individual learner)，再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质的(homogeneous)。同质集成中的个体学习器亦称“基学习器”(base learner)，相应的学习算法称为“基学习算法”(base learning algorithm)。

​	集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是“异质”的(heterogenous)。异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法；相应的，个体学习器一般不称为基学习器，常称为“组件学习器”(component learner)或直接称为个体学习器。

![集成学习示意图.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/AdaBoost/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%A4%BA%E6%84%8F%E5%9B%BE.png?raw=true)

​	集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对“弱学习器”(weak learner)尤为明显，因此集成学习的很多理论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。但需注意的是，虽然从理论上来说使用弱学习器集成足以获得好的性能，但在实践中出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器。

>弱学习器常指泛化性能略优于随机猜测的学习器；例如在二分类问题上精度略高于50%的分类器。

​	在一般经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比最坏的要好一些，比最好的要坏一些。集成学习把多个学习器结合起来，如何能获得比最好的单一学习器更好的性能呢？

​	考虑一个简单的例子：在二分类任务中，假定三个分类器在三个测试样本上的表现如下图所示，其中 $√$ 表示分类正确，$×$ 表示分类错误，集成学习的结果通过投票法(voting)产生，即“少数服从多数”。

​	在下图(a)中，每个分类器都只有 $66.6\%$ 的精度，但集成学习却达到了 $100\%$；在下图(b)中，三个分类器没有差别，集成之后性能没有提高；在下图(c)中，每个分类器的精度都只有33.3%，集成学习的结果变得更糟。这个简单的例子显示出：要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”(diversity)，即学习器间具有差异。

>个体学习器至少不差于弱学习器。

![集成个体应“好而不同”(h_i表示第i个分类器).png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/AdaBoost/%E9%9B%86%E6%88%90%E4%B8%AA%E4%BD%93%E5%BA%94%E2%80%9C%E5%A5%BD%E8%80%8C%E4%B8%8D%E5%90%8C%E2%80%9D(h_i%E8%A1%A8%E7%A4%BA%E7%AC%ACi%E4%B8%AA%E5%88%86%E7%B1%BB%E5%99%A8).png?raw=true)

​	我们来做个简单的分析。考虑二分类问题 $y∈\{-1,+1\}$ 和真实函数 $f$，假定基分类器的错误率为 $\epsilon$，即对每个基分类器 $h_i$ 有
$$
P(h_i(x)\neq f(x))=\epsilon\,\,.\tag{1}
$$
假设集成通过简单投票法结合 $T$ 个基分类器，若有超过半数的基分类器正确，则集成分类就正确：

>为简化讨论，假设 $T$ 为奇数。

$$
H(x)=sign\left(\sum^T_{i=1}h_i(x)\right)\,\,.\tag{2}
$$

​	假设基分类器的错误率相互独立，则由 $Hoeffding$ 不等式可知，集成的错误率为
$$
P(H(x)\neq f(x))=\sum^{[T/2]}_{k=0}\begin{pmatrix}T\\k\\\end{pmatrix}(1-\epsilon)^k\epsilon^{T-k}\leq exp\left(-\frac{1}{2}T(1-2\epsilon)^2\right)\,\,.\tag{3}
$$
上式显示出，随着集成中个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零。

​	然而我们必须注意到，上面的分析有一个关键假设：基学习器的误差相互独立。在现实任务中，个体学习器是为解决同一个问题训练出来的，它们显然不可能相互独立！事实上，个体学习器的“准确性”和“多样性”本身就存在冲突。一般地，准确性很高之后，要增加多样性就需牺牲准确性。事实上，**如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心**。

​	根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法；前者的代表是 $Boosting$，后者的代表是 $Bagging$ 和“ 随机森林”(Random Forest)。



## $Bagging$ 与随机森林

​	由个体与集成的知识可知，欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得好的集成，我们同时还希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器。为解决这个问题，我们可考虑使用相互有交叠的采样子集。



### $Bagging$

>$Bagging$ 这个名字是由 $Bootstrap\,\,AGGregatING$ 缩写而来。

​	$Bagging$ 是并行式集成学习方法最著名的代表。从名字即可看出，它直接基于自助采样法(bootstrap sampling)。给定包含 $m$ 个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过 $m$ 次随机采样操作，我们得到含 $m$ 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。它的特点是各个弱学习器之间没有依赖关系，可以并行拟合，而且是随机采样。

>随机采样就是从训练集里采集固定个数的样本，但是每采集一个样本后，都将样本放回。

​	照这样，我们可采样出 $T$ 个含 $m$ 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是 $Bagging$ 的基本流程。**在对预测输出进行结合时，$Bagging$ 通常对分类任务使用简单投票法，对回归任务使用简单平均法。**若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。$Bagging$ 的算法描述如下图所示。

>即每个基学习器使用相同权重的投票、平均。

![Bagging算法.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/RandomForest/Bagging%E7%AE%97%E6%B3%95.png?raw=true)

> $D_{bs}$ 是自助采样产生的样本分布。

​	

#### $Bagging$ 的优点：

- 训练一个 $Bagging$ 集成与直接使用基学习算法训练一个学习器的复杂度同阶，这说明 $Bagging$ 是一个很高效的集成学习算法。

  > 假定基学习器的计算复杂度为 $O(m)$，则 $Bagging$ 的复杂度大致为 $T(O(m)+O(s))$，考虑到采样与投票/平均过程的复杂度 $O(s)$很小，而 $T$ 通常是一个不太大的常数。

- 与标准 $AdaBoost$ 只适用于二分类任务不同，$Bagging$ 能不经修改地用于多分类、回归等任务。

- 自助采样过程还给 $Bagging$ 带来了另一个优点：由于每个基学习器只使用了初始训练集中约 $63.2\%$ 的样本，剩下约 $36.8\%$ 的样本可用作验证集来对泛化性能进行“包外估计”(out-of-bag estimate)。为此需记录每个基学习器所使用的训练样本。

- 由于 $Bagging$ 算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些。

不妨令 $D_t$ 表示 $h_t$ 实际使用的训练样本集，令 $H^{oob}(x)$ 表示对样本 $x$ 的包外预测，即仅考虑那些未使用 $x$ 训练的基学习器在 $x$ 上的预测，有
$$
H^{oob}(x)=\arg\limits_{y∈γ}\max\sum^T_{t=1}Ⅱ(h_t(x)=y)\,·\,Ⅱ(x\notin D_t)\,\,,\tag{1}
$$
则 $Bagging$ 泛化误差的包外估计为
$$
\epsilon^{oob}=\frac{1}{|D|}\sum_{(x,y)\in D}Ⅱ(H^{oob}(x)\neq y)\,\,.\tag{2}
$$
​	事实上，包外样本还有许多其他用途：

- 当基学习器是决策树时，可使用包外样本来辅助剪枝，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；
- 当基学习器是神经网络时，可使用包外样本来辅助早期停止以减小过拟合风险。



​	从偏差方差分解的角度看，$Bagging$ 主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。



### 随机森林

​	随机森林（Random Forest，简称RF）是 $Bagging$ 的一个扩展变体。$RF$ 在以决策树为基学习器构建 $Bagging$ 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合(假定有 $d$ 个属性)中选择一个最优属性；而在 $RF$ 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。

> 这里的参数 $k$ 控制了随机性的引入程度：若令 $k=d$，则基决策树的构建与传统决策树相同；若令 $k= 1$，则是随机选择一个属性用于划分；一般情况下，推荐值 $k= log_2d$。

​	随机森林简单、容易实现、计算开销小，令人惊奇的是，它在很多现实任务中展现出强大的性能，被誉为“代表集成学习技术水平的方法”。可以看出，随机森林对 $Bagging$ 只做了小改动，但是与 $Bagging$ 中基学习器的“多样性”仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。

​	随机森林的收敛性与 $Bagging$ 相似。如下图所示，随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时。这很容易理解，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。值得一提的是，随机森林的训练效率常优于 $Bagging$，因为在个体决策树的构建过程中，$Bagging$ 使用的是“确定型”决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的“随机型”决策树则只需考察一个属性子集。

![在两个UCI数据上,集成规模对随机森林与Bagging的影响.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/RandomForest/%E5%9C%A8%E4%B8%A4%E4%B8%AAUCI%E6%95%B0%E6%8D%AE%E4%B8%8A,%E9%9B%86%E6%88%90%E8%A7%84%E6%A8%A1%E5%AF%B9%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%8EBagging%E7%9A%84%E5%BD%B1%E5%93%8D.png?raw=true)



#### 随机森林的主要优点：

- 训练可以高度并行化。
- 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
- 在训练后，可以给出各个特征对于输出的重要性。
- 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
- 相对于 $Boosting$ 系列的 $Adaboost$ 和 $GBDT$ ，$RF$ 实现比较简单。
- 对部分特征缺失不敏感。

#### 随机森林的主要缺点：

- 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
- 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

>随机森林需要调整的参数有：
>
>- 决策树的个数
>- 特征属性的个数
>- 递归次数（决策树的深度）

---

#### 使用随机森林算法处理 $Kaggle$ 上的声呐数据集

##### 该项目分为如下步骤：

1. ##### 下载好数据

   [点击这里下载](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/)

2. #####  读取数据集csv文件

3. ##### 将特征转换为float类型

4. ##### 将数据集分成n份，用于交叉验证

5. ##### 随机采样

6. ##### 在指定特征个数下选取最优特征

7. ##### 构造决策树

8. ##### 创建随机森林

9. ##### 输入测试集并进行测试，输出预测结果

##### 训练部分：

​	取 dataset 中的 m 个 feature 来构造决策树，首先遍历 m 个 feature 中的每一个 feature，再遍历每一行，通过 spilt_loss 函数（计算分割代价）来选取最优的特征及特征值，根据是否大于这个特征值进行分类（分成left，right两类），循环执行上述步骤，直至不可分或者达到递归限值（用来防止过拟合），最后得到一个决策树 tree。

##### 测试部分：

​	对测试集的每一行进行判断，决策树 tree 是一个多层字典，每一层为一个二分类，将每一行按照决策树 tree 中的分类索引 index 一步一步向里层探索，直至不出现字典时探索结束，得到的值即为预测值。

---

Reference：《机器学习》