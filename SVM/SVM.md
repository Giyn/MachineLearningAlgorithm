[TOC]

# 什么是SVM？

支持向量机（support vector machine，简称为SVM）是分类与回归分析中的一种**监督学习算法**，也是一种**二分类模型**，其基本模型定义为特征空间上间隔最大的线性分类器，且基于最大间隔分隔数据，可转化为求解凸二次规划的问题。



## 间隔与支持向量

给定训练样本集 $D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\},y_i∈\{-1,+1\}$，（逻辑回归中负类标记是 $0$，与 $SVM$ 不同）分类学习最基本的想法就是基于训练集 $D$ 在样本空间中找到一个划分超平面，但是将不同类别的样本分开的划分超平面可能有很多，哪一个最合适呢？

![存在多个划分超平面将两类训练样本分开.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%E5%AD%98%E5%9C%A8%E5%A4%9A%E4%B8%AA%E5%88%92%E5%88%86%E8%B6%85%E5%B9%B3%E9%9D%A2%E5%B0%86%E4%B8%A4%E7%B1%BB%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E5%88%86%E5%BC%80.png?raw=true)

### 支持向量

在 $SVM$ 中，我们试图找到处于两类样本“正中间”的划分超平面，因为该划分超平面对训练数据局部扰动的容忍性最好。换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。

在样本空间中，划分超平面可通过如下线性方程来描述：
$$
w^Tx+b=0,
$$
其中 $w= (w_1;w_2;...;w_d)$ 为法向量（模型的权重向量），决定了超平面的方向；$b$ 为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量 $w$ 和位移 $b$ 确定，下面我们将其记为 $(w,b)$。样本空间中任意点 $x$ 到超平面 $(w,b)$ 的距离可写为：
$$
r = \frac{|w^Tx+b|}{||w||}.
$$

> ###### 样本空间中任意点 $x$ 到超平面 $(w,b)$ 的距离：
>
> 假设直线方程为 $ax_1+bx_2+c=0$，则由点到直线距离公式：
> $$
> r = \frac{|ax_1+bx_2+c|}{\sqrt{a^2+b^2}}
> $$
> 令 $w=(a,b)$，$x=(x_1,x_2)$，则可以把 $ax_1+bx_2$ 写成向量形式 $w^Tx$，把截距设为 $b$，则直线方程化为 $w^Tx+b=0$，代入上述公式，有：
> $$
> r=\frac{|w^Tx+b|}{\sqrt{w^Tw}}=\frac{|w^Tx+b|}{||w||}
> $$
> 该公式拓展到多维的情况下也同样适用。
>
> $ || w|| $ 就是向量 $w$ 的模。



假设超平面 $(w,b)$ 能将训练样本正确分类，即对于 $(x_i,y_i)∈D$，若 $y_i=+1$，则有 $w^Tx_i+b>0$；若 $y_i=-1$，则有 $w^Tx_i+b<0$。令
$$
\begin{cases}
w^Tx_i+b\geq+1,\quad{y_i=+1\,;}\\
w^Tx_i+b\leq-1,\quad{y_i=-1}\,.
\end{cases}
$$

> ###### 数学推导：
>
> 假设这个超平面是 $(w')^Tx +b'= 0$，对于 $(x_i,y_i)∈D$，有：
> $$
> \begin{cases}
> (w')^Tx_i +b'＞ 0,\quad{y_i=+1\,}\\
> (w')^Tx_i +b'＜0,\quad{y_i=-1}\,
> \end{cases}
> $$
> 根据几何间隔，将以上关系修正为：
> $$
> \begin{cases}
> (w')^Tx_i +b'\geq+ζ,\quad{y_i=+1\,}\\
> (w')^Tx_i +b'\leq-ζ,\quad{y_i=-1}\,
> \end{cases}
> $$
> 其中 $ζ$ 为某个大于零的常数，两边同除以 $ζ$，再次修正以上关系为：
> $$
> \begin{cases}
> (\frac{1}{ζ}w')^Tx_i + \frac{b}{ζ}'\geq+1,\quad{y_i=+1\,}\\
> (\frac{1}{ζ}w')^Tx_i + \frac{b}{ζ}'\leq-1,\quad{y_i=-1}\,
> \end{cases}
> $$
> 令：$w=\frac{1}{ζ}w'$，$b=\frac{b'}{ζ}$ ，则以上关系可写为：
> $$
> \begin{cases}
> w^Tx_i+b\geq+1,\quad{y_i=+1\,;}\\
> w^Tx_i+b\leq-1,\quad{y_i=-1}\,.
> \end{cases}
> $$



如下图所示，距离超平面最近的几个训练样本点使上式等号成立，它们被称为“支持向量”（样本空间中的数据点，由于一般写成向量形式，故称为支持向量），即下图中两条虚线上的点，两个异类支持向量到超平面的距离之和为：
$$
γ=\frac{2}{||w||},
$$
它被称为”间隔“。

![支持向量与间隔.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E4%B8%8E%E9%97%B4%E9%9A%94.png?raw=true)

在SVM中，我们希望实现的是 **最大化两类支持向量到超平面的距离之和**。欲找到具有“ 最大间隔”的划分超平面，也就是要找到能满足上式约束的参数 $w$ 和 $b$ ，使得 $γ$ 最大，即
$$
\max\limits_{w,b}\frac{2}{||w||}
$$

$$
s.t.\quad{}y_i(w^Tx_i+b)≥1,i= 1,2,...,m.
$$

> $s.t.$ 是约束条件的英语缩写。

>此时标记值 （$+1$ 或 $−1$）乘上预测值（$≥+1$ 或 $≤−1$）必定是一个 $\geq1$ 的数值。间隔貌似仅与 $w$ 有关，但事实上 $b$ 通过约束隐式地影响着 $w$ 的取值，进而对间隔产生影响。

为了最大化间隔，仅需最大化 $||w||^{-1}$，这等价于最小化 $||w||^2$。于是上式可重写为
$$
\min\limits_{w,b}\frac{1}{2}||w||^2\tag{1}
$$

$$
s.t.\quad{}y_i(w^Tx_i+b)\geq1,\quad{}i=1,2,...,m.
$$

> 引入 $\frac{1}{2}$ 是为了求导时可以约去平方项的 $2$。

这就是支持向量机的基本型。



> - 函数间隔：$y_i(w^Tx+b)$
> - 几何间隔：$\frac{y_i(w^Tx+b)}{||w||^2}$



## 对偶问题

我们希望求解式 $(1)$ 来得到大间隔划分超平面所对应的模型
$$
f(x)= w^Tx+b,
$$
其中 $w$ 和 $b$ 是模型参数。

注意到式 $(1)$ 本身是一个带约束的凸二次规划问题（凸问题就意味着必定能求到全局最优解，而不会陷入局部最优），能直接用现成的优化计算包求解，但我们可以有更高效的办法。



对式 $(1)$ 使用拉格朗日乘子法可得到其“对偶问题”。具体来说，对式 $(1)$ 的每条约束添加拉格朗日乘子 $α_i≥0$，则该问题的拉格朗日函数可写为
$$
L(w,b,α)=\frac{1}{2}||w||^2+\sum^m_{i=1}α_i(1-y_i(w^Tx_i+b))\,\,,\tag{2}
$$

> ###### 数学推导：
>
> 待求目标：
> $$
> \min\limits_x\quad{}f(x)
> $$
>
> $$
> s.t.\quad{}h(x)=0
> $$
>
> $$
> \quad{}\quad{}g(x)\leq0
> $$
>
> 等式约束和不等式约束：$h(x)=0,g(x)\leq0$ 分别是由一个等式方程和一个不等式方程组成的方程组。
>
> 拉格朗日乘子：$λ=(λ_1,λ_2,...,λ_m)\quad{}μ=(μ_1,μ_2,...,μ_n)$
>
> 拉格朗日函数：$L(x,λ,μ)=f(x)+λh(x)+μg(x)$



其中 $α=(α_1;α_2;...;a_m)$，令 $L(w,b,α)$ 对 $w$ 和 $b$ 的偏导为零可得
$$
w=\sum^m_{i=1}α_iy_ix_i\,\,,\tag{3}
$$

$$
0=\sum^m_{i=1}α_iy_i\,\,.\tag{4}
$$

> ###### 数学推导：
>
> 式 $(2)$ 可作如下展开：
> $$
> \begin{align}
> L(w,b,α)&=\frac{1}{2}||w||^2+\sum^m_{i=1}α_i(1-y_i(w^Tx_i+b))\\
> &=\frac{1}{2}||w||^2+\sum^m_{i=1}(α_i-α_iy_iw^Tx_i-α_iy_ib)\\
> &=\frac{1}{2}w^Tw+\sum^m_{i=1}α_i-\sum^m_{i=1}α_iy_iw^Tx_i-\sum^m_{i=1}α_iy_ib
> \end{align}
> $$
> 对 $w$ 和 $b$ 分别求偏导数并令其等于 $0$：
> $$
> \frac{∂L}{∂w}=\frac{1}{2}×2×w+0-\sum^m_{i=1}α_iy_ix_i-0=0\Longrightarrow{}w=\sum^m_{i=1}α_iy_ix_i\\\frac{∂L}{∂b}=0+0-0-\sum^m_{i=1}α_iy_i=0\Longrightarrow{}\sum_{i=1}^mα_iy_i=0
> $$



将式 $(3)$ 代入 $(2)$，即可将 $L(w,b,α)$ 中的 $w$ 和 $b$ 消去，再考虑式 $(4)$ 的约束，就得到式 $(1)$ 的对偶问题
$$
\max\limits_{α}\sum^m_{i=1}α_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}α_iα_jy_iy_jx_i^Tx_j\tag{5}
$$

$$
s.t.\quad{}\sum^m_{i=1}α_iy_i=0,\quad{}α_i\geq0,\quad{i=1,2,...,m\,\,.}
$$

> ###### 数学推导：
>
> 将式 $(3)$ 代入式 $(2)$，即可将 $L(w,b,α)$ 中的 $w$ 和 $b$ 消去，再考虑式 $(4)$ 的约束，就得到式 $(1)$ 的对偶问题：
> $$
> \begin{align}
> \min\limits_{w,b}L(w,b,α)&=\frac{1}{2}w^Tw+\sum^m_{i=1}α_i-\sum^m_{i=1}α_iy_iw^Tx_i-\sum^m_{i=1}α_iy_ib\\
> &=\frac{1}{2}w^T\sum^m_{i=1}α_iy_ix_i-w^T\sum^m_{i=1}α_iy_ix_i+\sum^m_{i=1}α_i-b\sum^m_{i=1}α_iy_i\\
> &=-\frac{1}{2}w^T\sum^m_{i=1}α_iy_ix_i+\sum^m_{i=1}α_i-b\sum^m_{i=1}α_iy_i
> \end{align}
> $$
> 又 $\sum\limits^m_{i=1}α_iy_i=0$，所以上式最后一项可化为0，于是有：
> $$
> \begin{align}
> \min\limits_{w,b}L(w,b,α)&=-\frac{1}{2}w^T\sum^m_{i=1}α_iy_ix_i+\sum^m_{i=1}α_i\\&=-\frac{1}{2}(\sum^m_{i=1}α_iy_ix_i)^T(\sum^m_{i=1}α_iy_ix_i)+\sum^m_{i=1}α_i\\&=-\frac{1}{2}\sum^m_{i=1}α_iy_ix_i^T\sum^m_{i=1}α_iy_ix_i+\sum^m_{i=1}α_i\\&=\sum_{i=1}^mα_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}α_iα_jy_iy_jx_i^Tx_j
> \end{align}
> $$
> 所以
> $$
> \max\limits_α\min\limits_{w,b}L(w,b,α)=\max\limits_α\sum^m_{i=1}α_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}α_iα_jy_iy_jx_i^Tx_j
> $$



解出 $α$ 后，求出 $w$ 与 $b$ 即可得到模型
$$
\begin{align}
f(x)&=w^Tx+b\\&=\sum^m_{i=1}α_iy_ix^T_ix+b\,\,.\tag{6}
\end{align}
$$

> 实际计算时一般不会直接求解 $α$，特别是需要用核函数映射到高维空间时，因为映射后做内积很困难，而用少量支持向量进行表示，在原始空间进行计算显然更优。

从对偶问题 $(5)$ 解出的 $α_i$ 是式 $(2)$ 中的拉格朗日乘子，它恰对应着训练样本 $(x_i,y_i)$。注意到式 $(1)$ 中有不等式约束，因此上述过程需满足 $KKT(Karush-Kuhn-Tucker)$ 条件，即要求
$$
\begin{cases}
α_i\geq0\,;\\
y_if(x_i)-1\geq0\,;\\
a_i(y_if(x_i)-1)=0\,.
\end{cases}\tag{7}
$$
即对任意训练样本 $(x_i,y_i)$，总有 $α_i=0$ 或 $y_if(x_i)=1$。

- 若 $α_i=0$，则该样本将不会在式 $(6)$ 的求和中出现，即不会影响到模型，也就不会对 $f(x)$ 有任何影响；

- 若 $α_i\,＞\,0$，则必有 $y_if(x_i)= 1$，此时所对应的样本点位于最大间隔边界上，是一个支持向量。

**这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关。**



### SMO算法

​	不难发现式 $(5)$ 是一个二次规划问题，可使用通用的二次规划算法来求解；然而，该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销。为了避开这个障碍，人们通过利用问题本身的特性，提出了很多高效算法，$SMO (Sequential-Minimal-Optimization)$ 是其中一个著名的代表。

#### SMO算法的基本思路

​	$SMO$ 的基本思路是先固定 $α_i$ 之外的所有参数，然后求 $α_i$ 上的极值。由于存在约束 $\sum^m_{i=1}α_iy_i=0$，若固定 $α_i$ 之外的其他变量，则 $α_i$ 可由其他变量导出。于是，$SMO$ 每次选择两个变量 $α_i$ 和 $α_j$，并固定其他参数。这样，在参数初始化后，$SMO$ 不断执行如下两个步骤直至收敛：

- 选取一对需更新的变量 $α_i$ 和 $α_j$；
- 固定 $α_i$ 和 $α_j$ 以外的参数，求解式 $(5)$ 获得更新后的 $α_i$ 和 $α_j$。



##### 怎么选取 $α_i$ 和 $α_j$ 呢？

​	注意到只需选取的 $α_i$ 和 $α_j$ 中有一个不满足 $KKT$ 条件，目标函数就会在迭代后减小。直观来看，$KKT$ 条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大。于是，$SMO$ 先选取违背 $KKT$ 条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，但由于比较各变量所对应的目标函数值减幅的复杂度过高，因此 **$SMO$ 采用了一个启发式：使选取的两变量所对应样本之间的间隔最大。一种直观的解释是，这样的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化，从而更快搜索到全局最大值。**

​	$SMO$ 算法之所以高效，恰由于在固定其他参数后，仅优化两个参数的过程能做到非常高效。具体来说，仅考虑 $α_i$ 和 $α_j$ 时，式 $(5)$ 中的约束可重写为
$$
α_iy_i+α_jy_j=c,\quad{}α_i\geq0,\,\,α_j\geq0\,\,,\tag{8}
$$
其中
$$
c=-\sum_{k≠i,j}α_ky_k
$$
是使 $\sum\limits^m_{i=1}α_iy_i=0$ 成立的常数，用
$$
α_iy_i+α_jy_j=c
$$
利用式 $(8)$ 消去式 $(5)$ 中的变量 $α_j$，则得到一个关于 $α_i$ 的单变量二次规划问题，仅有的约束是 $α_i\geq0$。不难发现，这样的二次规划问题具有闭式解，于是不必调用数值优化算法即可高效地计算出更新后的 $α_i$ 和 $α_j$。

使用SMO算法计算出最优解之后，我们关注的是如何推出 $w$ 和 $b$，从而得到最终模型。获得 $w$ 很简单，直接用式 $(3)$ 就可以了。

那么如何确定偏移项 $b$ 呢？注意到对任意支持向量 $(x_s,y_s)$ 都有 $y_sf(x_s)= 1$，即
$$
y_s\left(\sum_{i∈S}α_iy_ix_i^Tx_s+b\right)=1\,\,,\tag{9}
$$
其中 $S=\{i\midα_i＞0,\,i=1,2,...,m\}$ 为所有支持向量的下标集。理论上，可选取任意支持向量并通过求解式 $(9)$ 获得 $b$，但现实任务中常采用一种更鲁棒的做法：使用所有支持向量求解的平均值
$$
b=\frac{1}{|S|}\sum_{s∈S}\left(y_s-\sum_{i∈S}α_iy_ix_i^Tx_s\right)\,\,.
$$


## 核函数

### 非线性划分的处理

​	在前面的讨论中，我们假设训练样本是线性可分的，即存在一个划分超平面能将训练样本正确分类，然而在现实任务中，更常遇到的是在原始样本空间中非线性可分的问题，即原始样本空间内也许并不存在一个能正确划分两类样本的超平面。例如下图中的“异或”问题就不是线性可分的：

![异或问题与非线性映射.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%E5%BC%82%E6%88%96%E9%97%AE%E9%A2%98%E4%B8%8E%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84.png?raw=true)

​	对这样的问题，可**将样本从原始空间映射到一个更高维的特征空间**，使得样本在这个特征空间内线性可分。例如上图中，若将原始的二维空间映射到一个合适的三维空间，就能找到一个合适的划分超平面。幸运的是，如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。

​	令 $\phi(x)$ 表示将 $x$ 映射后的特征向量，于是，在特征空间中划分超平面所对应的模型可表示为
$$
f(x)=w^T\phi(x)+b\,,
$$
其中 $w$ 和 $b$ 是模型参数，类似式 $(1)$，有
$$
\min\limits_{w,b}\frac{1}{2}||w||^2\\
s.t.\quad{}y_i(w^T\phi(x_i)+b)\geq1,\quad{}i=1,2,...,m.
$$
​	其对偶问题是
$$
\max\limits_α\sum_{i=1}^mα_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}α_iα_jy_iy_j\phi(x_i)^T\phi(x_j)\\
s.t.\quad{}\sum^m_{i=1}α_iy_i=0\,,\,\,α_i\geq0,\quad{}i=1,2,...,m\,.\tag{10}
$$
​	求解式 $(10)$ 涉及到计算 $\phi(x_i)^T\phi(x_j)$，即样本 $x_i$ 与 $x_j$ 映射到高维特征空间之后的内积。

>  如 $x_i=(1,2,3),x_j=(4,5,6)$，故内积 $x_i^Tx_j$ 就等于 $1*4+2*5+3*6=32$。



### 什么是核函数

由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算 $\phi(x_i)^T\phi(x_j)$ 通常是困难的。为了避开这个障碍，就引入了**核函数（kernel function）**：
$$
\kappa(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle=\phi(x_i)^T\phi(x_j)\,,
$$

> $\langle\quad{}\rangle$ 是内积符号。

> 举个例子，假设输入空间是二维的，每个样本点有两个属性 $x$ 和 $y$，存在映射将每个样本点映射到三维空间：
> $$
> \phi(x)=\phi(x,y)=(x^2,\sqrt{2}xy,y^2)
> $$
> 给定原始空间中的两个样本点 $v_1=(x_1,y_1)$ 和 $v_2=(x_2,y_2)$，则它们映射到高维特征空间后的内积可以写作：
> $$
> \begin{align}
> \phi(v_1)^T\phi(v_2)&=\langle\phi(v_1),\phi(v_2)\rangle\\
> &=\langle(x_1^2,\sqrt{2}x_1y_1,y_1^2),(x_2^2,\sqrt{2}x_2y_2,y_2^2)\rangle\\
> &=x_1^2x_2^2+2x_1x_2y_1y_2+y_1^2y_2^2\\
> &=(x_1x_2+y_1y_2)^2\\
> &=\langle{}v_1,v_2\rangle^2\\
> &=\kappa(v_1,v_2)
> \end{align}
> $$
> 上述例子中，高维特征空间中两个点的内积，可以写成一个关于原始空间中两个点的函数 $\kappa(·,·)$ ，即为核函数。

> 随着原始空间维数增长，新空间的维数是呈爆炸性上升的。如果再进行映射，新特征空间的维度是难以想象的。



### 核函数的作用

即 $x_i$ 与 $x_j$ 在特征空间的内积等于它们在原始样本空间中通过函数 $\kappa(·,·)$ 计算的结果。有了这样的核函数，我们就不必直接去计算高维甚至无穷维特征空间中的内积，于是式 $(10)$ 可重写为
$$
\max\limits_α\sum_{i=1}^mα_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}α_iα_jy_iy_j\kappa(x_i,x_j)\\
s.t.\quad{}\sum^m_{i=1}α_iy_i=0\,,\,\,α_i\geq0,\quad{}i=1,2,...,m\,.
$$

> 这称为“核技巧”(ker-nel trick)。

求解后即可得到
$$
\begin{align}
f(x)&=w^T\phi(x)+b\\
&=\sum^m_{i=1}α_iy_i\phi(x_i)^T\phi(x)+b\\
&=\sum^m_{i=1}α_iy_i\kappa(x,x_i)+b\,\,.\tag{11}
\end{align}
$$
这里的函数 $\kappa(·,·)$ 就是”核函数“。式 $(11)$ 显示出模型最优解可通过训练样本的核函数展开，这一展式亦称“支持向量展式”(supportvector expansion)。

> 显然，在需要对新样本进行预测时，我们无需把新样本映射到高维（甚至无限维）空间，而是可以 利用保存下来的训练样本（支持向量）和核函数 $\kappa$ 进行求解。

> 核函数本身不等于映射，它只是一个与计算两个数据点映射到高维空间之后的内积等价的函数。

> 当我们发现数据在原始空间线性不可分时，会有把数据映射到高维空间来实现线性可分的想法，比方说引入原有属性的幂或者原有属性之间的乘积作为新的维度。

> 假设我们把数据点都映射到了一个维数很高甚至无穷维的特征空间，而模型求解和预测的过程需要用到映射后两个数据点的内积，这时直接计算就没辙了。但我们又幸运地发现，原来高维空间中两点的内积在数值上等于原始空间通过某个核函数算出的函数值，无需先映射再求值，就很好地解决了计算的问题了。



### 核函数的性质

​	显然，若已知合适映射 $\phi(·)$ 的具体形式，则可写出核函数 $\kappa(·,·)$，但在现实任务中我们通常不知道 $\phi(·)$ 是什么形式，合适的核函数是否一定存在呢？什么样的函数能做核函数呢？我们有下面的定理：

:o: 令 $\chi$ 为输入空间，$\kappa(·,·)$ 是定义在 $\chi×\chi$ 上的对称函数，则 $\kappa$ 是核函数当且仅当对于任意数据 $D=\{x_1,x_2,...,x_m\}$，”核矩阵“ $K$ 是一个规模为 $m×m$ 的函数矩阵，每个元素都是一个函数，它总是半正定的：
$$
K=\begin{bmatrix}
\kappa(x_1,x_1)&\cdots&\kappa(x_1,x_j)&\cdots&\kappa(x_1,x_m)\\
\vdots&\ddots&\vdots&\ddots&\vdots\\
\kappa(x_i,x_1)&\cdots&\kappa(x_i,x_j)&\cdots&\kappa(x_i,x_m)\\
\vdots&\ddots&\vdots&\ddots&\vdots\\
\kappa(x_m,x_1)&\cdots&\kappa(x_m,x_j)&\cdots&\kappa(x_m,x_m)
\end{bmatrix}\,.
$$
​	上述定理表明，**只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用**。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射 $\phi$。换言之，任何一个核函数都隐式地定义了一个称为“再生核希尔伯特空间”（Reproducing Kernel Hilbert Space，简称RKHS）的特征空间。

​	通过前面的讨论可知，我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。需注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。于是，“核函数选择”成为支持向量机的最大变数。若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。
$$
\begin{array}{c|lcr}
\hline
名称 & \text{表达式} & \text{参数}\\
\hline
线性核 & \kappa(x_i,x_j)=x_i^Tx_j & \\
多项式核 & \kappa(x_i,x_j)=(x_i^Tx_j)^d & d\geq1\,为多项式的次数\\
高斯核 & \kappa(x_i,x_j)=exp(-\frac{||x_i-x_j||^2}{2σ^2}) & σ＞0为高斯核的带宽（width）\\
拉普拉斯核 & \kappa(x_i,x_j)=exp(-\frac{||x_i-x_j||}{σ}) & σ＞0\\
Sigmoid核 & \kappa(x_i,x_j)=tanh(βx_i^Tx_j+θ) & tanh为双曲正切函数,β＞0,θ＜0\\
\hline
\end{array}
$$

> 文本数据一般用线性核**，**情况不明可尝试高斯核。

##### 除了上述常用的核函数，要产生核函数还可以使用函数组合的方式：

- 若 $\kappa_1$ 和 $\kappa_2$ 为核函数，则对于任意正数 $γ_1、γ_2$，其线性组合 $γ_1\kappa_1+γ_2\kappa_2$ 也是核函数。
- 若 $\kappa_1$ 和 $\kappa_2$ 为核函数，则核函数的直积 $\kappa_1⊗\kappa_2(x,z)=\kappa_1(x,z)\kappa_2(x,z)$ 也是核函数。
- 若 $\kappa_1$ 为核函数，则对于任意函数 $g(x)$，$\kappa(x,z)=g(x)\kappa_1(x,z)g(z)$ 也是核函数。



## 软间隔与正则化

​	在前面的讨论中，我们一直假定训练样本在样本空间或特征空间中是线性可分的，即存在一个超平面能将不同类的样本完全划分开。然而，在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分；退一步说，**即便恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果不是由于过拟合所造成的**。

​	缓解该问题的一个办法是**允许支持向量机在一些样本上出错**，此时的最大间隔更大，预测新样本时误分类的概率也会降低很多。为此，要引入“软间隔”（soft margin）的概念，如下图所示：

![软间隔示意图.红色圈出了一些不满足约束的样本..png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%E8%BD%AF%E9%97%B4%E9%9A%94%E7%A4%BA%E6%84%8F%E5%9B%BE.%E7%BA%A2%E8%89%B2%E5%9C%88%E5%87%BA%E4%BA%86%E4%B8%80%E4%BA%9B%E4%B8%8D%E6%BB%A1%E8%B6%B3%E7%BA%A6%E6%9D%9F%E7%9A%84%E6%A0%B7%E6%9C%AC..png?raw=true)

​	具体来说，前面介绍的支持向量机形式是要求所有样本均满足约束 $y_i(w^Tx_i+b)\geq1$，即所有样本都必须划分正确，这称为“硬间隔”（hard margin），而**软间隔则是允许某些样本不满足上述约束**。

当然，在最大化间隔的同时，不满足约束的样本应尽可能少。于是，优化目标可写为
$$
\min\limits_{w,b}\frac{1}{2}||w||^2+C\sum^m_{i=1}ℓ_{0/1}(y_i(w^Tx_i+b)-1)\,,\tag{12}
$$
其中 $C＞0$ 是一个常数，$ℓ_{0/1}$ 是”0/1损失函数“

> 常数 $C$ 一般取训练集大小的倒数 $(C=\frac{1}{m})$。

$$
ℓ_{0/1}(z)=
\begin{cases}
1,\quad{}if\,\,\,z＜0;\\
0,\quad{}otherwise.
\end{cases}
$$

即如果分类正确，那么函数间隔必定大于等于 $1$，此时损失为 $0$；如果分类错误，那么函数间隔必定小于等于 $-1$，此时损失为 $1$。

显然，当 $C$ 为无穷大时，式 $(12)$ 迫使所有样本均满足约束 $y_i(w^Tx_i+b)\geq1$，于是式 $(12)$ 等价于式 $(1)$；当 $C$ 取有限值时，式 $(12)$ 允许一些样本不满足约束。

​	然而，$ℓ_{0/1}$ 非凸、非连续，数学性质不太好，使得式 $(12)$ 不易直接求解。于是，人们通常用其他一些函数来代替 $ℓ_{0/1}$，称为“替代损失”（surrogate loss）。替代损失函数一般具有较好的数学性质，如它们通常是凸的连续函数且是 $ℓ_{0/1}$ 的上界。下图给出了三种常用的替代损失函数：
$$
hinge损失: ℓ_{hinge}(z) = max(0,1,-z);\\
指数损失(exponential-loss): l_{exp}(z) = exp(-z);\\
对率损失(logistic-loss): ℓ_{log}(z) = log(1 + exp(-z)).
$$
![三种常见的替代损失函数：hinge损失、指数损失、对率损失.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%E4%B8%89%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E6%9B%BF%E4%BB%A3%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9Ahinge%E6%8D%9F%E5%A4%B1%E3%80%81%E6%8C%87%E6%95%B0%E6%8D%9F%E5%A4%B1%E3%80%81%E5%AF%B9%E7%8E%87%E6%8D%9F%E5%A4%B1.png?raw=true)

实际任务中最常用的是 $hinge$ 损失，若采用 $hinge$ 损失，则式 $(12)$ 变成
$$
\min\limits_{w,b}\frac{1}{2}||w||^2+C\sum^m_{i=1}max(0,1-y_i(w^Tx_i+b))\,.\tag{13}
$$
引入“松弛变量”（slack variables）$ξ_i≥0$，可将式 $(13)$ 重写为
$$
\min\limits_{w,b,ξ_i}\frac{1}{2}||w||^2+C\sum^m_{i=1}ξ_i\tag{14}\\
s.t.\quad{}y_i(w^Tx_i+b)\geq1-ξ_i\,,\,\,\,\,\,ξ_i\geq0\,,\,\,i=1,2,...,m\,.
$$
这就是常用的“软间隔支持向量机”。

​	显然，式 $(14)$ 中每个样本都有一个对应的松弛变量，用以表征该样本不满足约束 $y_i(w^Tx_i+b)\geq1$（误分类）的程度，松弛变量的值越大，程度越高。



#### 求解软间隔支持向量机

>###### 式 $(14)$ 是一个二次规划问题，类似于前面的做法，分以下几步：
>
>1. 通过拉格朗日乘子法把 $m$ 个约束转换 $m$ 个拉格朗日乘子，得到该问题的拉格朗日函数。
>2. 分别对 $w,b,ξ$ 求偏导，代入拉格朗日函数得到对偶问题。
>3. 使用 $SMO$ 算法求解对偶问题，解出所有样本对应的拉格朗日乘子。
>4. 需要进行新样本预测时，使用支持向量及其对应的拉格朗日乘子进行求解。

式 $(14)$ 仍是一个二次规划问题，于是通过拉格朗日乘子法可得到式 $(14)$ 的拉格朗日函数
$$
\begin{align}
L(w,b,α,ξ,μ)=&\frac{1}{2}||w||^2+C\sum^m_{i=1}ξ_i\\&+\sum^m_{i=1}α_i(1-ξ_i-y_i(w^Tx_i+b))-\sum^m_{i=1}μ_iξ_i\,\,,\tag{15}
\end{align}
$$
由于式 $(14)$ 有两组各 $m$ 个不等式约束，因此该问题的拉格朗日函数有 $α_i≥0,μ_i≥0$ 两组拉格朗日乘子。

​	令 $L(w,b,α,ξ,μ)$ 对 $w,b,ξ_i$ 的偏导为零可得
$$
w=\sum^m_{i=1}α_iy_ix_i\,\,,\\
0=\sum^m_{i=1}α_iy_i\,\,,\\
C=α_i+μ_i\,\,.\tag{16}
$$
将式 $(16)$ 代入式 $(15)$ 即可得到式 $(14)$ 的对偶问题
$$
\max\limits_α\sum^m_{i=1}α_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}α_iα_jy_iy_jx_i^Tx_j\\
s.t.\quad{}\sum^m_{i=1}α_iy_i=0\,,\quad{}0\leqα_i\leq C\,,\,\,\,\,\,i=1,2,...,m\,\,.\tag{17}
$$
​	将式 $(17)$ 与硬间隔下的对偶问题 $(5)$ 对比可看出，两者唯一的差别就在于对偶变量的约束不同：前者是 $0≤α_i≤C$，后者是 $0≤α_i$。于是，可采用同样的算法求解式 $(17)$；在引入核函数后能得到与式 $(11)$ 同样的支持向量展式。

​	类似地，对软间隔支持向量机，$KKT$ 条件要求
$$
\begin{cases}
α_i\geq0,\quad{}μ_i\geq0\,\,,\\
y_if(x_i)-1+ξ_i\geq0\,\,,\\
α_i(y_if(x_i)-1+ξ_i)=0\,\,,\\
ξ_i\geq0,\quad{}μ_iξ_i=0\,\,.
\end{cases}
$$
于是对任意训练样本 $(x_i,y_i)$，总有 $α_i=0$ 或 $y_if(x_i)=1-ξ_i$。

- 若 $α_i=0$，则该样本不会对 $f(x)$ 有任何影响，不需要保留。

- 若 $α_i＞0$，则必有 $y_if(x_i)=1-ξ_i$，即该样本是支持向量。
- 由式 $(16)$ 可知：
  - 若 $α_i＜C$，则有 $μ_i>0$，进而有 $ξ_i=0$，此时函数间隔为 $1$，该样本恰好落在最大间隔边界上。
  - 若 $α_i=C$，则有 $μ_i=0$，因此松弛变量 $ξ_i>0$。
    - 若 $ξ_i\leq1$，则该样本落在最大间隔内部，被正确分类。
    - 若 $ξ_i＞1$，则该样本被错误分类。

由此可看出，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用 $hinge$ 损失函数仍保持了稀疏性。



#### 支持向量机和逻辑回归的联系与区别

​	那么，能否对式 $(12)$ 使用其他的替代损失函数呢？

​	可以发现，如果使用对率损失函数 $ℓ_{log}$ 来替代式 $(12)$ 中的 $0/1$ 损失函数，则几乎就得到了对率回归模型（逻辑回归模型）。实际上，支持向量机与对率回归的优化目标相近，通常情形下它们的性能也相当。对率回归的优势主要在于其输出具有自然的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，欲得到概率输出需进行特殊处理；此外，对率回归能直接用于多分类任务，支持向量机为此则需进行推广。另一方面，从上图可看出，$hinge$ 损失有一块“平坦”的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似支持向量的概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。

> ###### 支持向量机和逻辑回归的相同点：
>
> - 都是线性分类器，模型求解出一个划分超平面；
> - 两种方法都可以增加不同的正则化项；
> - 通常来说性能相当。
>
> ###### 支持向量机和逻辑回归的不同点：
>
> - $LR$ 使用对率损失，$SVM$ 一般用 $hinge$ 损失；
>
> - 在 $LR$ 的模型求解过程中，每个训练样本都对划分超平面有影响，影响力随着与超平面的距离增大而减小，所以说 $LR$ 的解受训练数据本身的分布影响；$SVM$ 的模型只与占训练数据少部分的支持向量有关，所以说，$SVM$ 不直接依赖数据分布，所得的划分超平面不受某一类点的影响；
> - 如果数据 类别不平衡比较严重，$LR$ 需要先做相应处理再训练，$SVM$ 则不用；
> - $SVM$ 依赖于数据表达的距离测度，需要先把数据标准化，$LR$ 则不用（但实际任务中可能会为了方便选择优化过程的初始值而进行标准化）。如果数据的距离测度不明确（特别是高维数据），那么最大间隔可能就变得没有意义；
> - $LR$ 的输出有概率意义，$SVM$ 的输出则没有；
> - $LR$ 可以直接用于多分类任务，$SVM$ 则需要进行扩展（但更常用one-vs-rest）；
> - $LR$ 使用的对率损失是光滑的单调递减函数，无法导出支持向量，解依赖于所有样本，因此预测开销较大；$SVM$ 使用的 $hinge$ 损失有“零区域”，因此 解具有稀疏性（书中没有具体说明这句话的意思，但按我的理解是解出的拉格朗日乘子 $α$ 具有稀疏性，而不是权重向量 $w$，从而不需用到所有训练样本。
>
> ###### 在实际运用中，$LR$ 更常用于大规模数据集，速度较快；$SVM$ 适用于规模小，维度高的数据集。

> ###### Andrew NG：
>
> - 如果Feature的数量很大，跟样本数量差不多，这时候选用 LR 或者是 Linear Kernel 的SVM；
> - 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用 SVM + Gaussian Kernel；
> - 如果Feature的数量比较小，而样本数量很多，需要手工添加一些 Feature 变成第一种情况。



#### 正则化

​	我们还可以把式 $(12)$ 中的 $0/1$ 损失函数换成别的替代损失函数以得到其他学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项 $\sum^m_{i=1}ℓ(f(x_i),y_i)$ 用来表述训练集上的误差，可写为更一般的形式
$$
\min\limits_fΩ(f)+C\sum^m_{i=1}ℓ(f(x_i),y_i),\tag{18}
$$
其中 $Ω(f)$ 称为“结构风险”（structural risk），**用于描述模型 $f$ 的某些性质**；第二项 $\sum^m_{i=1}ℓ(f(x_i),y_i)$ 称为“经验风险”（empirical risk），**用于描述模型与训练数据的契合程度**；$C$ 用于对二者进行折中，权衡这两种风险。

> 其他模型大多都是在最小化经验风险的基础上，再考虑结构风险（避免过拟合）。**SVM却是从最小化结构风险来展开的**。

- 从经验风险最小化的角度来看，$Ω(f)$ 表述了我们希望获得具有何种性质的模型（例如希望获得复杂度较小的模型），这为引入领域知识和用户意图提供了途径；
- 另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，式 $(18)$ 称为“正则化”（regularization）问题，$Ω(f)$ 称为正则化项，$C$ 则称为正则化常数。

> 正则化可理解为一种“罚函数法”，即对不希望得到的结果施以惩罚，从而使得优化过程趋向于希望目标。从贝叶斯估计的角度来看，正则化项可认为是提供了模型的先验概率。

$L_p$ 范数（norm）是常用的正则化项，其中 $L_2$ 范数 $||w||_2$ 倾向于 $w$ 的分量取值尽量均衡，即非零分量个数尽量稠密，而 $L_0$ 范数 $||w||_0$ 和 $L_1$ 范数 $||w||_1$ 则倾向于 $w$ 的分量尽量稀疏，即非零分量个数尽量少。



## 支持向量回归

​	现在我们来考虑回归问题。给定训练样本 $D = \{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\},y_i∈R$，希望学得一个形如 $f(x)= w^Tx+b$ 的回归模型，使得 $f(x)$ 与 $y$ 尽可能接近，而不是像分类任务那样，旨在令不同类的预测值可以被划分开，$w$ 和 $b$ 是待确定的模型参数。

​	对样本 $(x, y)$，传统回归模型通常直接基于模型输出 $f(x)$ 与真实输出 $y$ 之间的差别来计算损失，当且仅当 $f(x)$ 与 $y$ 完全相同时，损失才为零。

与此不同，支持向量回归（Support Vector Regression，简称SVR）假设我们能容忍 $f(x)$ 与 $y$ 之间最多有 $\epsilon$ 的偏差，即仅当 $f(x)$ 与 $y$ 之间的差别绝对值大于 $\epsilon$ 时才计算损失。如下图所示，这相当于以 $f(x)$ 为中心，构建了一个宽度为 $2\epsilon$ 的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。

![支持向量回归示意图.红色显示出ε-间隔带,落入其中的样本不计算损失..png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92%E7%A4%BA%E6%84%8F%E5%9B%BE.%E7%BA%A2%E8%89%B2%E6%98%BE%E7%A4%BA%E5%87%BA%CE%B5-%E9%97%B4%E9%9A%94%E5%B8%A6,%E8%90%BD%E5%85%A5%E5%85%B6%E4%B8%AD%E7%9A%84%E6%A0%B7%E6%9C%AC%E4%B8%8D%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1..png?raw=true)

于是，$SVR$ 问题可形式化为
$$
\min\limits_{w,b}\frac{1}{2}||w||^2+C\sum^m_{i=1}ℓ_\epsilon(f(x_i)-y_i)\,\,,\tag{19}
$$
其中 $C$ 为正则化常数，$ℓ_\epsilon$ 是下图所示的 $\epsilon$-不敏感损失（$\epsilon$ -insensitive loss）函数
$$
ℓ_\epsilon(z)=
\begin{cases}
0,\qquad{}\qquad{}if\,\,|z|\leq \epsilon\,\,;\\
|z|-\epsilon,\qquad{}otherwise.
\end{cases}
$$
![ε-不敏感损失函数.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/SVM/%CE%B5-%E4%B8%8D%E6%95%8F%E6%84%9F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png?raw=true)

引入松弛变量 $ξ_i$ 和 $\hat{ξ_i}$，分别表示间隔带两侧的松弛程度，间隔带两侧的松弛程度可有所不同。此时可将式 $(19)$ 重写为
$$
\min\limits_{w,b,ξ_i,\hat{ξ_i}}\frac{1}{2}||w||^2+C\sum^m_{i=1}(ξ_i+\hat{ξ_i})\tag{20}\\
\begin{align}
s.t.\quad{}&f(x_i)-y_i\leq\epsilon+ξ_i\,\,,\\
&y_i-f(x_i)\leq\epsilon+\hat{ξ_i}\,\,,\\
&ξ_i\geq0,\,\,\hat{ξ_i}\geq0,\,\,i=1,2,...,m.
\end{align}
$$
​	类似式 $(15)$，此处有四组 $m$ 个约束条件，故通过引入拉格朗日乘子 $μ_i≥0$，$\hat{μ_i}≥0$，$α_i≥0$，$\hat{α_i}≥0$，由拉格朗日乘子法可得到式 $(20)$ 的拉格朗日函数
$$
\begin{align}
&L(w,b,α,\hat{α},ξ,\hat{ξ},μ,\hat{μ})\\
&=\frac{1}{2}||w||^2+C\sum^m_{i=1}(ξ_i+\hat{ξ_i})-\sum^m_{i=1}{μ_i}{ξ_i}-\sum^m_{i=1}\hat{μ_i}\hat{ξ_i}\\
&+\sum^m_{i=1}α_i(f(x_i)-y_i-\epsilon-ξ_i)+\sum^m_{i=1}\hat{α_i}(y_i-f(x_i)-\epsilon-\hat{ξ_i})\,\,.
\end{align}\tag{21}
$$
​	将 $f(x)=w^Tx+b$ 代入，再令 $L(w,b,α,\hat{α},ξ,\hat{ξ},μ,\hat{μ})$ 对 $w$，$b$，$ξ_i$ 和 $\hat{ξ_i}$ 的偏导为零可得
$$
w=\sum^m_{i=1}(\hat{α_i}-α_i)x_i\,\,,\tag{22}
$$

$$
0=\sum^m_{i=1}(\hat{α_i}-α_i)\,\,,\tag{23}
$$

$$
C=α_i+μ_i\,\,,\tag{24}
$$

$$
C=\hat{α_i}+\hat{μ_i}\,\,.\tag{25}
$$

​	将式 $(22)$ - $(25)$ 代入式 $(21)$，即可得到 $SVR$ 的对偶问题
$$
\begin{align}
\max\limits_{w,b,ξ_i,\hat{ξ_i}}&\sum^m_{i=1}y_i(\hat{α_i}-α_i)-\epsilon(\hat{α_i}-α_i)\\
&-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}(\hat{α_i}-α_i)-(\hat{α_j}-α_j)x^T_ix_j\\
s.t.\quad{}&\sum^m_{i=1}(\hat{α_i}-α_i)=0\,\,,\\
& 0\leqα_i,\hat{α_i}\leq C\,\,.
\end{align}\tag{26}
$$
上述过程中需满足 $KKT$ 条件，即要求
$$
\begin{cases}
α_i(f(x_i)-y_i-\epsilon-ξ_i)=0\,\,,\\
\hat{α_i}(y_i-f(x_i)-\epsilon-\hat{ξ_i})=0\,\,,\\
α_i\hat{α_i}=0\,\,,\,\,ξ_i\hat{ξ_i}=0\,\,,\\
(C-α_i)ξ_i=0\,\,,\,\,(C-\hat{α_i})\hat{ξ_i}=0\,\,.
\end{cases}\tag{27}
$$
可以看出：

- 当且仅当 $f(x_i)-y_i-\epsilon-ξ_i = 0$ 时 $α_i$ 能取非零值；
- 当且仅当 $y_i-f(x_i)-\epsilon-\hat{ξ_i} = 0$ 时 $\hat{α_i}$ 能取非零值。

换言之，仅当样本 $(x_i, y_i)$ 不落入 $\epsilon$-间隔带中，相应的 $α_i$ 和 $\hat{α_i}$ 才能取非零值。此外，约束 $f(x_i)-y_i-\epsilon-ξ_i = 0$ 和 $y_i-f(x_i)-\epsilon-\hat{ξ_i} = 0$ 不能同时成立，因此 $α_i$ 和 $\hat{α_i}$ 中至少有一个为零。

> 落在 $\epsilon$-间隔带中的样本都满足 $α_i=0$ 且 $\hat{α_i}=0$。

​	将式 $(22)$ 代入 $f(x)=w^Tx+b$，则 $SVR$ 的解形如
$$
f(x)=\sum^m_{i=1}(\hat{α_i}-α_i)x^T_ix+b\,\,.\tag{28}
$$
​	能使式 $(28)$ 中的 $(\hat{α_i}-α_i)\neq0$ 的样本即为 $SVR$ 的支持向量，它们必落在 $\epsilon$-间隔带之外。显然，$SVR$ 的支持向量仅是训练样本的一部分，即其解仍具有稀疏性。

​	由 $KKT$ 条件 $(27)$ 可看出，对每个样本 $(x_i, y_i)$ 都有 $(C-α_i)ξ_i=0$ 且 $α_i(f(x_i)-y_i-\epsilon-ξ_i) = 0$。于是，在得到 $α_i$ 后，若 $0＜α_i＜C$，则必有 $ξ_i=0$，进而有
$$
b=y_i+\epsilon-\sum^m_{i=1}(\hat{α_i}-α_i)x_i^Tx\,\,.\tag{29}
$$
因此，在求解式 $(26)$ 得到 $α_i$ 后，理论上来说，可任意选取满足 $0<α_i< C$ 的样本通过式 $(29)$ 求得 $b$。实践中常采用一种更鲁棒的办法：选取多个（或所有）满足条件 $0<α_i< C$ 的样本求解 $b$ 后取平均值。


​		若考虑特征映射形式 $f(x)=w^T\phi(x)+b$，则相应的，式 $(22)$ 将形如
$$
w=\sum^m_{i=1}(\hat{α_i}-α_i)\phi(x_i)\,\,.\tag{30}
$$
将式 $(30)$ 代入 $f(x)=w^T\phi(x)+b$，则 $SVR$ 可表示为
$$
f(x)=\sum^m_{i=1}(\hat{α_i}-α_i)\kappa(x,x_i)+b\,\,,\tag{31}
$$
其中 $\kappa(x_i,x_j)=\phi(x_i)^T\phi(x_j)$ 为核函数。



## 核方法

​	回顾式 $(11)$ 和 $(31)$ 可发现，给定训练样本 $\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$，若不考虑偏移项 $b$，则无论 $SVM$ 还是 $SVR$，学得的模型总能表示成核函数 $\kappa(x,x_i)$ 的线性组合。不仅如此，事实上我们有下面这个称为“表示定理”（representer theorem）的更一般的结论：

:o: 令 $H$ 为核函数 $\kappa$ 对应的再生核希尔伯特空间，$||h||_H$。表示 $H$ 空间中关于 $h$ 的范数，对于任意单调递增函数 $Ω:[0,∞]→R$ 和任意非负损失函数 $ℓ:R^m→[0,∞]$，优化问题
$$
\min\limits_{h∈H}F(h)=Ω(||h||_H)+ℓ(h(x_1),h(x_2),...,h(x_m))\tag{32}
$$
的解总可写为
$$
h^*(x)=\sum^m_{i=1}α_i\kappa(x,x_i)\,\,.\tag{33}
$$
​	表示定理对损失函数没有限制，对正则化项 $Ω$ 仅要求单调递增，甚至不要求 $Ω$ 是凸函数，意味着对于一般的损失函数和正则化项，优化问题 $(32)$ 的最优解 $h^*(x)$ 都可表示为核函数 $\kappa(x,x_i)$ 的线性组合；这显示出核函数的巨大威力。

​	人们发展出一系列基于核函数的学习方法，统称为“核方法”（kernelmethods）。最常见的，是通过“核化”（即引入核函数）来将线性学习器拓展为非线性学习器。

​	下面我们以线性判别分析为例来演示如何通过核化来对其进行非线性拓展，从而得到“核线性判别分析”（Kernelized Linear DiscriminantAnalysis，简称KLDA）。

​	我们先假设可通过某种映射 $\phi:\chi→F$ 将样本映射到一个特征空间 $F$，然后在 $F$ 中执行线性判别分析，以求得
$$
h(x)=w^T\phi(x)\,\,.\tag{34}
$$
$KLDA$ 的学习目标是
$$
\max\limits_wJ(w)=\frac{w^TS^{\phi}_bw}{w^TS^{\phi}_ww}\,\,,\tag{35}
$$
其中 $S^{\phi}_b$ 和 $S^{\phi}_w$ 分别为训练样本在特征空间 $F$ 中的类间散度矩阵和类内散度矩阵。令 $X_i$ 表示第 $i∈\{0,1\}$ 类样本的集合，其样本数为 $m_i$；总样本数 $m=m_o+m_1$。第 $i$ 类样本在特征空间 $F$ 中的均值为
$$
μ^{\phi}_i=\frac{1}{m_i}\sum_{x∈X_i}\phi(x)\,\,,\tag{36}
$$
两个散度矩阵分别为
$$
S^{\phi}_b=(μ_1^{\phi}-μ_0^{\phi})(μ_1^{\phi}-μ_0^{\phi})^T\,\,;\tag{37}
$$

$$
S^{\phi}_w=\sum^1_{i=0}\sum_{x∈X_i}(\phi(x)-μ^{\phi}_i)(\phi(x)-μ^{\phi}_i)^T\,\,.\tag{38}
$$

​	通常我们难以知道映射 $\phi$ 的具体形式，因此使用核函数 $\kappa(x,x_i)=\phi(x_i)^T\phi(x)$ 来隐式地表达这个映射和特征空间 $F$。把 $J(w)$ 作为式 $(32)$ 中的损失函数 $ℓ$，再令 $Ω= 0$，由表示定理，函数 $h(x)$ 可写为
$$
h(x)=\sum^m_{i=1}α_i\kappa(x,x_i)\,\,,\tag{39}
$$
于是由式 $(34)$ 可得
$$
w=\sum^m_{i=1}α_i\phi(x_i)\,\,.\tag{40}
$$
​	令 $K∈R\,^{m×m}$ 为核函数 $\kappa$ 所对应的核矩阵，$(K)_{ij}= \kappa(x_i,x_j)$。令 $1_i∈\{1,0\}^{m×1}$ 为第 $i$ 类样本的指示向量，即 $1_i$ 的第 $j$ 个分量为 $1$ 当且仅当 $x_j∈X_i$，否则 $1_i$ 的第 $j$ 个分量为0。再令
$$
\hat{μ}_0=\frac{1}{m_0}K1_0\,\,,\tag{41}
$$

$$
\hat{μ}_1=\frac{1}{m_1}K1_1\,\,,\tag{42}
$$

$$
M=(\hat{μ}_0-\hat{μ}_1)(\hat{μ}_0-\hat{μ}_1)^T\,\,,\tag{43}
$$

$$
N=KK^T-\sum^1_{i=0}m_i\hat{μ}_i\hat{μ}_i^T\,\,.\tag{44}
$$

​	于是，式 $(35)$ 等价为
$$
\max\limits_αJ(α)=\frac{α^TMα}{α^TNα}\,\,.\tag{45}
$$
​	显然，使用线性判别分析求解方法即可得到 $α$，进而可由式 $(39)$ 得到投影函数 $h(x)$。

---

Reference：《机器学习》
