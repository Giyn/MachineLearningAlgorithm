[TOC]

# 什么是决策树？

决策树是一种逻辑简单的机器学习算法，可用作分类，也可用作回归，属于监督学习（Supervised learning）。



## 基本流程

以西瓜问题为例，**决策树是基于树形结构来进行决策的**，要对“是否是好瓜”进行决策，我们通常会进行一系列的“子决策”，从色泽、根蒂和敲声进行判断，进而得出最终决策。

![西瓜问题的一颗决策树.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/Decision%20Tree/%E8%A5%BF%E7%93%9C%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%80%E9%A2%97%E5%86%B3%E7%AD%96%E6%A0%91.png?raw=true)

**一般地，一棵决策树可以分成三个部分：一个根结点、若干个内部结点和若干个叶结点。**叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。

**决策就是从根节点开始走到叶节点的过程。**每经过一个节点的判定，数据集就按照分裂特征的阈值划分为若干子集，在子节点做判定时只需要考虑对应的数据子集即可。

决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。且决策树的生成是一个递归过程，其遵循简单且直观的“分而治之”（divide-and-conquer）策略：

![决策树学习基本算法.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/Decision%20Tree/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95.png?raw=true)



**在决策树基本算法中，有三种情形会导致递归返回：**

（1）当前结点包含的样本全属于同一类别，无需划分；
（2）当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
（3）当前结点包含的样本集合为空，不能划分。

在第（2）种情形下，把当前结点标记为叶结点，将其类别设定为该结点所含样本最多的类别；           在第（3）种情形下，把当前结点标记为叶结点，将其类别设定为其父结点所含样本最多的类别。

注意：两种情形的处理实质不同：情形（2）是在利用当前结点的后验分布，而情形（3）则是把父结点的样本分布作为当前结点的先验分布。



## 划分选择

**决策树算法中，选择最优划分属性是关键。**一般来说，随着结点不断分类，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。



### 信息增益

“信息熵”是度量样本集合纯度最常用的一种指标，假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为$P_k (k=1,2,...,|y|)$，则 $D$ 的信息熵定义为：
$$
Ent(D)=-\sum^{|y|}_{k=1}p_klog_2p_k
$$
$Ent(D)$ 的值越小，则 $D$ 的纯度越高。



假定离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1,a^2, ...,a^V\}$ ，若使用 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$ 。我们可根据式（4.1）计算出 $D^v$ 的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重 $|D^v|/|D|$，即样本数越多的分支结点对提升纯度的帮助越大，于是可计算出用属性 $a$ 对样本集 $D$ 进行划分所获得的“信息增益”，**求和项也称为“条件熵”**：
$$
Gain(D,a)=Ent(D)-\sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)
$$
一般地，信息增益越大，则使用属性 $a$ 来进行划分所获得的“纯度提升”越大。因此可用信息增益来进行决策树的划分属性选择，即在决策树算法中选择属性 $a_* = arg_{a∈A}max Gain(D,a)$。

**$ID3 $ 算法就是以信息增益为准则来选择划分属性。**



### 信息增益率

信息增益准则对可取值数目较多的属性有所偏好，然而这样的决策树显然不具有泛化能力，无法对新样本进行有效预测。而 $C4.5$ 算法不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性，信息增益率定义为：
$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum^V_{v=1}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$
称为属性 $a$ 的“固有值”。它的定义与信息熵类似，信息熵衡量的是样本集在类别上的混乱程度，而 **固有值衡量的是样本集在某个属性上的混乱程度**。若属性 $a$ 的可能取值数目 $V$ 越大，$IV(a)$ 的值通常会越大，即该属性混乱程度越高。

需要注意的是，信息增益率准则对可取值数目较少的属性有所偏好。因此，$C4.5$ 算法并不是直接选择信息增益率最大的候选划分属性，而是使用了一个启发式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。



### 基尼指数

$CART$ 算法使用“基尼指数”来选择划分属性。

>CART（Classification and Regression Tree，即分类回归树算法）是一种著名的决策树学习算法，可用于分类和回归任务。

数据集 $D$ 的纯度可用基尼值来度量：
$$
Gini(D)=\sum^{|y|}_{k=1}\sum_{k'≠k}p_kp_{k'}=\sum^{|y|}_{k=1}p_k(1-p_k)=1-\sum^{|y|}_{k=1}p^2_k
$$
直观来说，**$Gini(D)$ 反映了从数据集 $D$ 中随机抽取 $2$ 个样本，其类别标记不一致的概率**。因此，$Gini(D) $ 越小，基尼值越小，则数据集 $D$ 的纯度越高。属性 $α$ 的基尼指数定义：
$$
Gini\_index(D,a)=\sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)
$$
基尼指数越小，表示使用属性 $a$ 划分后纯度的提升越大。因此，在属性集合 $A$ 中，选择基尼指数最小的属性 $a$ 作为最优划分属性，即 $a_*=arg_{a∈A}minGini\_index(D,a)$。



## 剪枝处理

剪枝（pruning）是决策树学习算法应对“过拟合”的主要手段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多，这时就可能因训练样本学得“太好” 了，以致于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。



**决策树剪枝的基本策略有 “预剪枝” 和 “后剪枝”**。

#### 预剪枝

- 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；

#### 后剪枝

- 后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。

>实际任务中，即使没有提升，只要不是性能下降，一般也会剪枝，因为根据奥卡姆剃刀准则，简单的模型更好。

> 只有一层划分（即只有根节点一个非叶节点）的决策树称为 **决策树桩（decision stump）**。



#### 优缺点

- **预剪枝**

  预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。

- **后剪枝**

  后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。



## 连续与缺失值

#### 连续值处理

前面讨论了基于离散属性来生成决策树，而现实中常会遇到连续属性。由于连续属性的可取值数目不再有限，所以需要对连续属性离散化。最简单的策略是采用二分法对连续属性进行处理，这正是 $C4.5$ 算法中采用的机制。



给定样本集 $D$ 和连续属性 $a$，假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值，将这些值从小到大进行排序，记为 $\{a^1,a^2,...,a^n\}$ 。基于划分点 $t$ 可将 $D$ 分为子集 $D_t^-$ 和 $D_t^+$ ，其中 $D_t^-$ 为在属性 $a$ 上取值不大于 $t$ 的样本，而 $D_t^+$ 为在属性 $a$ 上取值大于 $t$ 的样本。显然，对相邻的属性取值 $a^i$ 与 $a^{i+1}$ 来说，$t$ 在区间 $[a^i,a^{i+1})$ 中取任意值所产生的划分结果相同。因此，对连续属性 $a$，我们可以考察包含 $n-1$ 个元素的候选划分点集合
$$
T_a=\{\frac{a^i+a^{i+1}}{2}\mid1≤i≤n-1\}
$$
即把区间 $[a^i,a^{i+1})$ 的中位点 $\frac{a^i+a^{i+1}}{2}$ 作为候选划分点，继而按照一定的准则考察这 $n-1$ 个划分点，选取最优的划分点进行样本集合的划分，例如按照信息增益准则：
$$
Gain(D,a)=max_{t∈T_a}Gain(D,a,t)\\=max_{t∈T_a}Ent(D)-\sum_{λ∈\{-,+\}}\frac{|D_t^λ|}{|D|}Ent(D_t^λ)
$$
其中 $Gain(D,a,t)$ 是样本集 $D$ 基于划分点 $t$ 二分后的信息增益。于是，我们就可选择使 $Gain(D, a, t)$ 最大化的划分点进行样本集合的划分。

>和离散属性不同，连续属性用于当前节点的划分后，其后代节点依然可以使用该连续属性进一步划分。



#### 缺失值处理

在属性数目较多的情况下，往往会有大量样本出现缺失值。现实任务中常会遇到某些属性值缺失的不完整样本。如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，显然是对数据信息极大的浪费。

**缺失值处理需解决两个问题：**

1. ###### 如何在属性值缺失的情况下进行划分属性选择？

给定训练集 $D$ 和属性 $a$，令 $\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集。我们仅可根据 $\tilde{D}$ 来判断属性 $a$ 的优劣。假定属性 $a$ 有 $V$ 个可取值 $\{a^1,a^2,...,a^V\}$，令 $\tilde{D}^v$ 表示 $\tilde{D}$ 中在属性 $a$ 上取值为 $a^v$ 的样本子集，$\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第 $k$ 类 $(k= 1,2,...,|γ|)$ 的样本子集，则显然有 $\tilde{D} =\cup^{|γ|}_{k=1}\tilde{D}_k,\tilde{D}=∪^V_{v=1}\tilde{D}^v$。假定我们为每个样本 $x$ 赋予一个权重 $w_x$，并定义
$$
ρ=\frac{\sum_{x∈\tilde{D}}w_x}{\sum_{x∈{D}}w_x},\qquad\qquad\qquad\qquad
$$

$$
\tilde{p}_k=\frac{\sum_{x∈\tilde{D}_k}w_x}{\sum_{x∈{D}}w_x}\qquad(1\leq{k}\leq{|γ|}),
$$

$$
\tilde{r}_v=\frac{\sum_{x∈\tilde{D}_v}w_x}{\sum_{x∈{D}}w_x}\qquad(1\leq{v}\leq{V}).
$$

对属性 $a$，$ρ$ 表示无缺失值样本所占的比例，$\tilde{p}_k$ 表示无缺失值样本中第 $k$ 类所占的比例，$\tilde{r}_v$ 则表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例。显然，$\sum^{|γ|}_{k=1}\tilde{p}_k=1$，$\sum^V_{v=1}\tilde{r}_v=1$.

> 注意，这里的 $w_x$ 表示样本的权值，它是含缺失值样本参与建模的一种方式。在根节点处初始时，所有样本 $x$ 的权重都为 $1$。

**基于上述定义，可将信息增益的计算式推广为：**
$$
Gain(D,a)=ρ×Gain(\tilde{D},a)\\=ρ×(Ent(\tilde{D})-\sum^{V}_{v=1}\tilde{r}_vEnt(\tilde{D}^v)),
$$
其中 $Ent(\tilde{D})=-\sum^{|γ|}_{k=1}\tilde{p}_klog_2\tilde{p}_k$

按照该推广来计算包含缺失值的属性的信息增益，然后和其他属性的信息增益相比，选出最优的。



2. ###### 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子结点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\tilde{r}_v·w_x$；直观地看，这就是让同一个样本以不同的概率划入到不同的子结点中去。

> 可以把无缺失值的决策树建模想象为各样本权值恒为 $1$ 的情形，它们只对自己所属的属性值子集作贡献。而样本含缺失值时，它会以不同的概率对所有属性值子集作贡献。$C4.5$ 算法便使用了上述解决方案。



## 多变量决策树

若把每个属性视为坐标空间中的一个坐标轴，则 $d$ 个属性描述的样本就对应了 $d$ 维空间中的一个数据点；对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。决策树所形成的分类边界有一个明显的特点：轴平行，即它的分类边界由若干个与坐标轴平行的分段组成。

![决策树对应的分类边界.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/Decision%20Tree/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AF%B9%E5%BA%94%E7%9A%84%E5%88%86%E7%B1%BB%E8%BE%B9%E7%95%8C.png?raw=true)

前面提到的决策树都是单变量决策树， 即在每个节点处做判定时都只用到一个属性。它有一个特点，就是形成的分类边界都是轴平行的。显然，这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。

但在学习任务的真实分类边界比较复杂时，必须画出很多超平面（线），在预测时就需要继续大量的属性测试（遍历决策树）才能得到结果，预测时间开销很大。若能使用斜的划分边界，则决策树模型将大为简化。

“多变量决策树”就是能实现这样的“斜划分”甚至更复杂划分的决策树。以实现斜划分的多变量决策树为例，在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试；换言之，每个非叶结点是一个形如 $\sum^d_{i=1}w_ia_i=t$ 的线性分类器，其中 $w_i$ 是属性 $a_i$ 的权重，$w_i$ 和 $t$ 可在该结点所含的样本集和属性集上学得。于是，与传统的“单变量决策树”不同，**在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。**

![决策树对复杂分类边界的分段近似.png](https://github.com/Giyn/QGSummerTraining/blob/master/Pictures/Decision%20Tree/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AF%B9%E5%A4%8D%E6%9D%82%E5%88%86%E7%B1%BB%E8%BE%B9%E7%95%8C%E7%9A%84%E5%88%86%E6%AE%B5%E8%BF%91%E4%BC%BC.png?raw=true)



# 决策树算法的优劣

from sklearn Decision Trees summary:

##### 优势：

- 易理解，解释性好，易可视化；
- 数据预处理少；
- 复杂度O(logN)；
- 支持标称变量 + 连续变量；
- 支持多输出；
- 白盒模型，布尔逻辑；
- 模型好坏易验证；
- 容忍先验知识错；

##### 劣势：

- 决策树生成易太大、过拟合；（需要剪枝、设置树最大深度等后续操作）
- 模型生成不稳定，易受小错误样本影响；
- 学习最优模型是N-P难题，贪心搜索易陷入局部最优；（可采用随机初始化生成多个模型）
- 不支持非线性逻辑，例如XOR；
- 数据不平衡时生成的树形差；

---

Reference：《机器学习》