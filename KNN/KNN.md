[TOC]

# 降维与度量学习

## $k$ 近邻学习

​		$k$ 近邻 (k-Nearest Neighbor，简称kNN) 学习是一种常用的监督学习方法，其工作机制非常简单：给定测试样本，基于某种距离度量找出训练集中与其最靠近的 $k$ 个训练样本，然后基于这 $k$ 个“邻居”的信息来进行预测。通常，在分类任务中可使用“投票法”，即选择这 $k$ 个样本中出现最多的类别标记作为预测结果；在回归任务中可使用“平均法”，即将这 $k$ 个样本的实值输出标记的平均值作为预测结果；还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。

​		与前面介绍的学习方法相比，$k$ 近邻学习有一个明显的不同之处：它似乎没有显式的训练过程！事实上，它是“懒惰学习” (lazy learning) 的著名代表，此类学习技术在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习” (eager learning)。

​		下图给出了 $k$ 近邻分类器的一个示意图。显然，$k$ 是一个重要参数，当 $k$ 取不同值时，分类结果会有显著不同。另一方面，若采用不同的距离计算方式，则找出的“近邻”可能有显著差别，从而也会导致分类结果有显著不同。

![k近邻分类器示意图.虚线显示出等距线;测试样本在k=1或k=5时被判别为正例,k=3时被判别为反例](../../Pictures/KNN/k%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E5%99%A8%E7%A4%BA%E6%84%8F%E5%9B%BE.%E8%99%9A%E7%BA%BF%E6%98%BE%E7%A4%BA%E5%87%BA%E7%AD%89%E8%B7%9D%E7%BA%BF;%E6%B5%8B%E8%AF%95%E6%A0%B7%E6%9C%AC%E5%9C%A8k=1%E6%88%96k=5%E6%97%B6%E8%A2%AB%E5%88%A4%E5%88%AB%E4%B8%BA%E6%AD%A3%E4%BE%8B,k=3%E6%97%B6%E8%A2%AB%E5%88%A4%E5%88%AB%E4%B8%BA%E5%8F%8D%E4%BE%8B.png)

​		暂且假设距离计算是“恰当”的，即能够恰当地找出 $k$ 个近邻，我们来对“最近邻分类器” ($1NN$，即 $k=1$) 在二分类问题上的性能做一个简单的讨论。

​		给定测试样本 $x$，若其最近邻样本为 $z$，则最近邻分类器出错的概率就是 $x$ 与 $z$ 类别标记不同的概率，即
$$
P(err)=1-\sum_{c∈\gamma}P(c\mid x)P(c\mid z)\,\,.\tag{1}
$$
​		假设样本独立同分布，且对任意 $x$ 和任意小正数 $δ$，在 $x$ 附近 $δ$ 距离范围内总能找到一个训练样本；换言之，对任意测试样本，总能在任意近的范围内找到式 (1) 中的训练样本 $z$。令 $c^*= arg\,max_{c∈\gamma}P(c\mid x)$ 表示贝叶斯最优分类器的结果，有
$$
\begin{align}
P(err)&=1-\sum_{c∈\gamma}P(c\mid x)P(c\mid z)\\
&\simeq1-\sum_{c∈\gamma}P^2(c\mid x)\\
&\leq1-P^2(c^*\mid x)\\
&=(1+P(c^*\mid x))(1-P(c^*\mid x))\\
&\leq2×(1-P(c^*\mid x))\,\,.\tag{2}
\end{align}
$$
于是我们得到了有点令人惊讶的结论：最近邻分类器虽简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍！



### $KNN$ 优缺点：

#### 优点：

- 简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；
- 可用于数值型数据和离散型数据；
- 训练时间复杂度为 $O(n)$；无数据输入假定；
- 对异常值不敏感。

#### 缺点：

- 计算复杂性高；空间复杂性高；
- 样本不平衡问题；
- 一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。
- 最大的缺点是无法给出数据的内在含义。



## 低维嵌入

​		上述讨论是基于一个重要假设：任意测试样本 $x$ 附近任意小的 $δ$ 距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为“密采样” (dense sample)。然而，这个假设在现实任务中通常很难满足，例如若 $δ = 0.001$，仅考虑单个属性，则仅需 $1000$ 个样本点平均分布在归一化后的属性取值范围内，即可使得任意测试样本在其附近 $0.001$ 距离范围内总能找到一个训练样本，此时最近邻分类器的错误率不超过贝叶斯最优分类器的错误率的两倍。然而，这仅是属性维数为 $1$ 的情形，若有更多的属性，则情况会发生显著变化。例如假定属性维数为 $20$，若要求样本满足密采样条件，则至少需 $(10^3)^{20}= 10^{60}$ 个样本。现实应用中属性维数经常成千上万，要满足密采样条件所需的样本数目是无法达到的天文数字。此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大的麻烦，例如当维数很高时甚至连计算内积都不再容易. 

​		事实上，在高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难” (curse ofdimensionality)。

​		缓解维数灾难的一个重要途径是降维 (dimension reduction)，亦称“维数约简”，即通过某种数学变换将原始高维属性空间转变为一个低维“子空间”(subspace), 在这个子空间中样本密度大幅提高，距离计算也变得更为容易.为什么能进行降维？这是因为在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维“ 嵌入” (embedding)。下图给出了一个直观的例子。原始高维空间中的样本点，在这个低维嵌入子空间中更容易进行学习。

![低维嵌入示意图](../../Pictures/KNN/%E4%BD%8E%E7%BB%B4%E5%B5%8C%E5%85%A5%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

​		若要求原始空间中样本之间的距离在低维空间中得以保持，如上图所示，即得到“多维缩放” (Multiple Dimensional Scaling，简称 $MDS$ ) 这样一种经典的降维方法。下面做一个简单的介绍。
​		假定 $m$ 个样本在原始空间的距离矩阵为 $D∈\mathbb{R}^{m×m}$，其第 $i$ 行 $j$ 列的元素 $dist_{ij}$ 为样本 $x_i$ 到 $x_j$ 的距离。我们的目标是获得样本在 $d'$ 维空间的表示 $Z∈\mathbb{R}^{d'×m},d'≤d$，且任意两个样本在 $d'$ 维空间中的欧氏距离等于原始空间中的距离，即 $||z_i-z_j||=dist_{ij}$。
​		令 $B=Z^TZ∈\mathbb{R}^{mxm}$，其中 $B$ 为降维后样本的内积矩阵，$b_{ij}=z_i^Tz_j$，有
$$
\begin{align}
dist_{ij}^2&=||z_i||^2+||z_j||^2-2z_i^Tz_j\\
&=b_{ii}+b_{jj}-2b_{ij}\,\,.\tag{3}
\end{align}
$$

> $0∈\mathbb{R}^{d'}$ 为全零向量。

​		为便于讨论，令降维后的样本 $Z$ 被中心化，即 $\sum^m_{i=1}z_i=0$。显然，矩阵 $B$ 的行与列之和均为零，即 $\sum^m_{i=1}b_{ij}=\sum^m_{j=1}b_{ij}0$。易知
$$
\sum^m_{i=1}dist^2_{ij}=tr(B)+mb_{jj}\,\,,\tag{4}
$$

$$
\sum^m_{j=1}dist^2_{ij}=tr(B)+mb_{ii}\,\,,\tag{5}
$$

$$
\sum^m_{i=1}\sum^m_{j=1}dist^2_{ij}=2m\,tr(B)\,\,,\tag{6}
$$

其中 $tr(·)$ 表示矩阵的迹 (trace)，$tr(B)=\sum^m_{i=1}||z_i||^2$。令
$$
dist_{i.}^2=\frac{1}{m}\sum^m_{j=1}dist_{ij}^2\,\,,\tag{7}
$$

$$
dist_{.j}^2=\frac{1}{m}\sum^m_{j=1}dist_{ij}^2\,\,,\tag{8}
$$

$$
dist_{..}^2=\frac{1}{m^2}\sum^m_{i=1}\sum^m_{j=1}dist_{ij}^2\,\,,\tag{9}
$$

由式 (3) 和式 (4)~(9) 可得
$$
b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist^2_{i.}-dist^2_{.j}+dist^2_{..})\,\,,\tag{10}
$$
由此即可通过降维前后保持不变的距离矩阵 $D$ 求取内积矩阵 $B$。

​		对矩阵 $B$ 做特征值分解 (eigenvalue decomposition)，$B=Ⅴ\Lambda Ⅴ^T$，其中 $\Lambda= diag(λ_1,λ_2,...,\lambda_d)$ 为特征值构成的对角矩阵，$λ_1≥\lambda_2≥...≥\lambda_d$，$V$ 为特征向量矩阵。假定其中有 $d^*$ 个非零特征值，它们构成对角矩阵 $\Lambda_*=diag(\lambda_1,\lambda_2,...,\lambda_{d^*})$。令 $Ⅴ_*$ 表示相应的特征向量矩阵，则 $Z$ 可表达为
$$
Z=\Lambda_*^{1/2}Ⅴ^T_*∈\mathbb{R}^{d^*×m}\,\,.\tag{11}
$$
​		在现实应用中为了有效降维，往往仅需降维后的距离与原始空间中的距离尽可能接近，而不必严格相等。此时可取 $d'\ll d$ 个最大特征值构成对角矩阵 $\tilde{\Lambda}=diag(\lambda_1,\lambda_2,...,\lambda_{d'})$，令 $\tilde{Ⅴ}$ 表示相应的特征向量矩阵，则 $Z$ 可表达为
$$
Z=\tilde{\Lambda}^{1/2}\tilde{Ⅴ}^T∈\mathbb{R}^{d'×m}\,\,.\tag{12}
$$
下图给出了 $MDS$ 算法的描述：

![MDS算法](../../Pictures/KNN/MDS%E7%AE%97%E6%B3%95.png)

​		一般来说，欲获得低维子空间，最简单的是对原始高维空间进行线性变换。给定 $d$ 维空间中的样本 $X=(x_1,x_2,...,x_m)∈\mathbb{R}^{d×m}$，变换之后得到 $d'≤d$ 维空间中的样本
$$
Z=W^TX\,\,,\tag{13}
$$

> 通常令 $d'\ll d$。

其中 $W∈\mathbb{R}^{d×d'}$ 变换矩阵，Z∈Rd"xm是样本在新空间中的表达。
		变换矩阵 $W$ 可视为 $d'$ 个 $d$ 维基向量，$z_i=W^Tx_i$ 是第 $i$ 个样本与这 $d'$ 个基向量分别做内积而得到的 $d'$ 维属性向量。换言之，$z_i$ 是原属性向量 $x_i$ 在新坐标系 $\{w_1,w_2,...,w_{d'}\}$ 中的坐标向量。若 $w_i$ 与 $w_j(i≠j)$ 正交，则新坐标系是一个正交坐标系，此时 $W$ 为正交变换。显然，新空间中的属性是原空间中属性的线性组合。
		基于线性变换来进行降维的方法称为线性降维方法，它们都符合式 (13) 的基本形式，不同之处是对低维子空间的性质有不同的要求，相当于对 $W$ 施加了不同的约束。在下一节我们将会看到，若要求低维子空间对样本具有最大可分性,则将得到一种极为常用的线性降维方法。
		对降维效果的评估，通常是比较降维前后学习器的性能，若性能有所提高则认为降维起到了作用。若将维数降至二维或三维，则可通过可视化技术来直观地判断降维效果。

---

Reference：《机器学习》